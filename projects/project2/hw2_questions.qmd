---
title: "Poisson Regression Examples"
author: "Warren Kennedy"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available.

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

### Data

To begin our analysis, we first need to read in the data.

```{r}
# read in the data
blueprinty <- read.csv("~/My_Quarto_Website/projects/project2/blueprinty.csv")
```

We want to use this data to provide insights as to whether or not firms using Blueprinty's software are more successful in getting their patent applications approved. We first visualize our data to see if there is an obvious difference in the number of patents approved between customers and non-customers. The plot below shows a roughly similar distribution between both groups. One notable difference is that, for customers, we see more volume centered around 4 while the non-customers' number of patents approved seems to be equally likely to be between 0 and 4. We cannot be certain looking at the plots alone, so we continue by looking at the average number of patents approved given  customer status.

```{r}
customers <- blueprinty[blueprinty$iscustomer == 1, ]
noncustomers <- blueprinty[blueprinty$iscustomer == 0, ]

# Plot the first histogram
hist(noncustomers$patents, 
     col = "blue", 
     main = "Histogram Comparison", 
     xlab = "Number of Patents", 
     ylab = "Frequency",
     ylim = c(0, max(hist(noncustomers$patents)$counts, hist(customers$patents)$counts))
)

# Add the second histogram to the same plot
hist(customers$patents, 
     col = "red", 
     add = TRUE
)

# Add a legend
legend("topright", 
       legend = c("Non-Customers", "Customers"), 
       fill = c("blue", "red"))

```

The means for both groups are listed below. The mean for patents approved is roughly the same between customers and non-customers, but we can not be sure if there is a statistically significant difference between the two groups by simply comparing the means. To get more insights we can use linear regression to test the effect of being a customer.

```{r}
#split data into customer and non-customer
customers <- blueprinty[blueprinty$iscustomer == 1, ]
noncustomers <- blueprinty[blueprinty$iscustomer == 0, ]

patent_mean_c <- mean(customers$patents) 
patent_mean_nc <- mean(noncustomers$patents)

print(patent_mean_c)
print(patent_mean_nc)
```

Our linear model estimates that there is indeed a positive causal effect from being a customer on the number of patents that are approved. This finding is statistically significant at the 95 percent level.

```{r}
patents_lm <- lm(patents~iscustomer, data=blueprinty)
summary(patents_lm)
```

Despite our findings, it is important to note that the Blueprinty customers may not be selected at random. There may be systematic differences in the age and regional location of customers vs non-customers. To check whether or not there is a systematic difference in the composition of the group, we will look at the group level compositions for both customers and non-customers. In the plots below, we compare the distributions for age and region.

#### Comparing Regions

Looking at the distribution of region for customers and nun-customers, we see a distribution that is mostly similar. Most of the members in the data are either located in the Northeast or the Southwest, with each of the three remaining region seemingly being equally proportioned among the remaining data. Given the similar distributions for the groups, we do not see evidence of a systematic difference between customers and non-customers when comparing regions.

```{r}
# Get frequency counts for each group in column_name1
group_counts1 <- table(noncustomers$region)

# Plot a bar plot of the frequency counts for column_name1
barplot(group_counts1,
        main = "Region Frequency Comparison",
        xlab = "Regions",
        ylab = "Frequency",
        col = "blue",
        border = "black"
)

# Get frequency counts for each group in column_name2
group_counts2 <- table(customers$region)

# Add the bar plot of the frequency counts for column_name2 to the existing plot
barplot(group_counts2,
        col = "red",
        add = TRUE
)

# Add a legend
legend("topright",
       legend = c("Non-Customer", "Customer"),
       fill = c("blue", "red")
)


```

#### Comparing Ages

When comparing the ages between the customer and non-customer groups we see a difference between the distribution of ages. The age data for customers is skewed right, while the data for non-customers is shaped more like a bell curve. This difference translates to the customer group being younger on average than the non-customer group. With this difference in mind, we have reason to believe that there may be a systematic difference between the customer and non-customer group that we will likely want to account for prior to drawing any conclusions.

```{r}
# Plot the first histogram
hist(noncustomers$age, 
     col = "blue", 
     main = "Histogram Comparison", 
     xlab = "Age", 
     ylab = "Frequency",
     ylim = c(0, max(hist(noncustomers$age)$counts, hist(customers$age)$counts))
)

# Add the second histogram to the same plot
hist(customers$age, 
     col = "red", 
     add = TRUE
)

# Add a legend
legend("topright", 
       legend = c("Non-Customers", "Customers"), 
       fill = c("blue", "red"))

```

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density plot to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The mathematical likelihood for a Poisson distribution is as follows: $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

Next, we plot the Poisson likelihood (as well as the log-likelihood) as a function of \lambda using the number of patents observed as the input for Y.

```{r}
poisson_log_likelihood <- function(lambda, Y){
   log_likelihood <- sum(Y * log(lambda) - lambda - log(factorial(Y)))
   return(sum(log_likelihood))
}

```

```{r}
# Use the patents column from the blueprinty dataset
Y <- blueprinty$patents

# Create a sequence of lambda values
lambda_values <- seq(0.1, 20, by = 0.1)  # Start from 0.1 to avoid lambda = 0

# Calculate log-likelihood for each lambda value using the vector of patents
log_likelihood_values <- sapply(lambda_values, function(lambda) poisson_log_likelihood(lambda, Y))

# Plot log-likelihood
plot(lambda_values, log_likelihood_values, type = "l", col = "red", xlab = "Lambda", ylab = "Log-Likelihood", main = "Poisson Log-Likelihood", ylim = range(log_likelihood_values, na.rm = TRUE))


```


We can find the maximum likelihood estimate (MLE) of ( $\lambda$) for the Poisson distribution by optimizing the log-likelihood function using the `optim()` function. Here's how we can do it:

```{r}
# Define the log-likelihood function for the Poisson distribution
poisson_log_likelihood <- function(lambda, Y){
   -sum(dpois(Y, lambda, log = TRUE))
}

# Use the patents column from the blueprinty dataset
Y <- blueprinty$patents

# Use optim() to find the MLE
mle_result <- optim(par = 1, fn = poisson_log_likelihood, Y = Y, lower = 0.01, upper = 20, method = "Brent")

# Extract the MLE
lambda_mle <- mle_result$par
lambda_mle

```

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{r}
# Convert region to factor
blueprinty$region <- as.factor(blueprinty$region)
blueprinty$region <- relevel(blueprinty$region, ref = "Midwest")

# Create covariate matrix X
X <- model.matrix(~ age + I(age^2) + iscustomer + region, data=blueprinty)
y <- blueprinty$patents

# Update likelihood function for Poisson regression
poisson_reg_ll <- function(beta, X, y){
   lambda <- exp(X %*% beta)
   log_likelihood <- sum(dpois(y, lambda, log = TRUE))
  return(log_likelihood)
}

```

```{r}
## Use optim() to find the MLE vector and the Hessian
out <- optim(par = rep(0, ncol(X)), fn = poisson_reg_ll, X = X, y = y, hessian = TRUE, control = list(fnscale = -1))


## Calculate standard errors of beta parameter estimates
Hinv <- -solve(out$hessian)
results <- cbind(coefs = out$par, sterr = sqrt(diag(Hinv)))
rownames(results) <- colnames(X)
results <- round(results, 4)

## Display coefficients and standard errors
results
```

We can test the validity of our results using a generalized linear model.

```{r}
# Fit a Poisson regression model using glm
glm_model <- glm(patents ~ age + I(age^2) + region + iscustomer, data = blueprinty, family = "poisson")

# Display summary of the glm model
summary(glm_model)

```

Interpreting the results of our regression we can cnofirm that being a customer has a positive and statistically significant impact on the likelihood of a patent application being approved. This finding supports the initial claim that firms using Blueprinty's software are more successful in getting their patent applications approved. Our regression results are also telling us that age and region have a statistically significant effect on patent outcomes. This finding is consistent with the differences we had observed in patent outcomes and age/region distributions between customers and non customers. Given our results, it would be wise for a firm to consider becoming a customer to realize the effects of Blueprinty's software on patent success.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:

::: {.callout-note collapse="true"}
### Variable Definitions

```         
- `id` = unique ID number for each unit
- `last_scraped` = date when information scraped
- `host_since` = date when host first listed the unit on Airbnb
- `days` = `last_scraped` - `host_since` = number of days the unit has been listed
- `room_type` = Entire home/apt., Private room, or Shared room
- `bathrooms` = number of bathrooms
- `bedrooms` = number of bedrooms
- `price` = price per night (dollars)
- `number_of_reviews` = number of reviews for the unit on Airbnb
- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
- `review_scores_location` = a "quality of location" score from reviews (1-10)
- `review_scores_value` = a "quality of value" score from reviews (1-10)
- `instant_bookable` = "t" if instantly bookable, "f" if not
```
:::

### Data

To begin our analysis, we first need to read in the data. At first glance, our data has a large amount of 'NA' values. If the missing values were few and randomly distributed, we could choose to simply remove the rows with NA values. However in this case, we would lose a significant amount of data if we were to resolve the missing data in this way. Instead we have chosen to replace the missing data with the most common values from their respective columns.

```{r}
# read in the data
airbnb <- read.csv("~/My_Quarto_Website/projects/project2/airbnb.csv")

# Function to replace NA with mode
na_to_mode <- function(x) {
  if (is.factor(x) | is.character(x)) {
    mode_val <- as.character(which.max(table(x)))
    x[is.na(x)] <- mode_val
  } else {
    mode_val <- as.numeric(names(which.max(table(x))))
    x[is.na(x)] <- mode_val
  }
  return(x)
}

# Apply the function to each column
airbnb_clean <- as.data.frame(lapply(airbnb, na_to_mode))

```

#### Visualizing the Response Variable

For this data we use `number_of_reviews` as a proxy for a room being booked. To begin our analysis, we will visualize our data to see if there are any notable trends. The first plot below shows the distribution for the number of reviews variables. As expected, our data is right-skewed, indicating that most rooms have a higher probability of having few reviews as opposed to many.  
```{r}
library(ggplot2)
ggplot(airbnb_clean, aes(x = airbnb_clean$number_of_reviews)) +
  geom_histogram()

max(airbnb_clean$number_of_reviews)
```

The second trend that we note is that the number_of_reviews is positively correlated with the number of days a unit has been listed. This trend is to be expected given that the higher the number of days that a unit is listed, the more time the unit has to be given a review. 
```{r}
# Find the 10th percentile of the "days" variable
percentile_10 <- quantile(airbnb_clean$days, 0.1)

# Subset the data to include only the lowest 90 percent of the "days" variable
subset_df <- airbnb_clean[airbnb_clean$days <= percentile_10, ]

ggplot(subset_df, aes(x = days, y = number_of_reviews)) +
  geom_point() +  # Add points
  geom_line()   
```

Next we take a look at the difference between units that offer an instant booking option against those that don't. The mean values for the two groups are different, so we will want to compare the overall composition of the groups to see if they are any adjustments that need to be made for our analysis.
```{r}
#split data into customer and non-customer
instant_book <- airbnb_clean[airbnb_clean$instant_bookable == 't', ]
no_instant_book <- airbnb_clean[airbnb_clean$instant_bookable == 'f', ]

review_mean_ib <- mean(instant_book$number_of_reviews) 
review_mean_nonib <- mean(no_instant_book$number_of_reviews)

print(review_mean_ib)
print(review_mean_nonib)
```

In the plot below, we see that the number of reviews can vary significantly depending on the room type. Entire homes and apartments tend to receive the most reviews while shared rooms hardly receive any. This difference in outcomes can be used to compare the distributions between Instant booking and non-instant booking units.
```{r}
# Create a bar plot
ggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews)) +
  geom_bar(stat = "identity", fill = "dodgerblue") +
  labs(x = "Room Type", y = "Number of Reviews", title = "Number of Reviews by Room Type")
```


When we compare the room types between both groups, we see that they are relatively the same. We will continue our analysis assuming that the two groups have a similar composition. 
```{r}
# Get frequency counts for each group in column_name1
group_counts1 <- table(no_instant_book$room_type)

# Plot a bar plot of the frequency counts for column_name1
barplot(group_counts1,
        main = "Room_Type Comparison",
        xlab = "Room Type",
        ylab = "Frequency",
        col = "dodgerblue",
        border = "black"
)

# Get frequency counts for each group in column_name2
group_counts2 <- table(instant_book$room_type)

# Add the bar plot of the frequency counts for column_name2 to the existing plot
barplot(group_counts2,
        col = "red",
        add = TRUE
)

# Add a legend
legend("topright",
       legend = c("No Instant Book", "Instant Book"),
       fill = c("dodgerblue", "red")
)
```

### Modeling the data

The Poisson regression model explores the factors influencing the number of reviews for Airbnb listings. The intercept (5.537) represents the expected number of reviews when all other variables are zero, which is not practically meaningful in this context. For each additional day of availability, the expected number of reviews increases by a factor of 1.00005021. Private rooms are associated with a 14.57% decrease, and shared rooms with a 41.71% decrease in the expected number of reviews compared to entire home/apartment listings. Each additional bathroom is linked to a 11.61% decrease, while each additional bedroom corresponds to a 7.02% increase in the expected number of reviews. Higher prices are associated with a slight decrease in reviews, with each unit increase leading to a 0.024% decrease. Higher review scores for cleanliness, location, and value are associated with increases in the expected number of reviews. Lastly, listings that are instant bookable have an expected 34.68% increase in reviews compared to non-instant bookable listings.

```{r}
model <- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price + review_scores_cleanliness +review_scores_location + review_scores_value + instant_bookable, data = airbnb_clean , family = poisson)

summary(model)
```

### Estimation of Poisson Regression Model

Next, we confirm our results using a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. 

```{r}
# Convert region to factor
airbnb_clean$room_type <- as.factor(airbnb_clean$room_type)
airbnb_clean$room_type <- relevel(airbnb_clean$room_type, ref = "Shared room")

# Create covariate matrix X
X2 <- model.matrix(~ ., data=airbnb_clean)
y2 <- airbnb_clean$number_of_reviews

# Update likelihood function for Poisson regression
poisson_reg_ll <- function(beta, X2, y2){
   lambda <- exp(X2 %*% beta)
   log_likelihood <- sum(dpois(y2, lambda, log = TRUE))
  return(log_likelihood)
}

```

```{r}
## Use optim() to find the MLE vector and the Hessian
#out <- optim(par = rep(0, ncol(X2)), fn = poisson_reg_ll, X = X2, y = y2, hessian = TRUE, control = list(fnscale = -1), method = 'BFGS')


## Calculate standard errors of beta parameter estimates
#Hinv <- -solve(out$hessian)
#results <- cbind(coefs = out$par, sterr = sqrt(diag(Hinv)))
#rownames(results) <- colnames(X2)
#results <- round(results, 4)

## Display coefficients and standard errors
#results
```

