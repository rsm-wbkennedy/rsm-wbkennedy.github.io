[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Warren Kennedy",
    "section": "",
    "text": "Warren Kennedy is an Operations Analyst at Midland Credit Management, an Encore Capital Group subsidiary. When not innovating on data platforms, Warren enjoys watching sports, walking his dogs, and overpaying for popcorn at movie theatres.\n\nEducation\nUC San Diego Rady School of Management | San Diego, CA\n\nMaster of Science in Business Analytics | August 2023 - December 2024\nBachelor of Arts in Economics | September 2021 - June 2023\n\n\n\nExperience\nMidland Credit Management | Operations Analyst | August 2023 - Present"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Warren Kennedy’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nWarren Kennedy\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Demand: An Analysis of Different Methods\n\n\n\n\n\n\nWarren Kennedy\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nWarren Kennedy\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nWarren Kennedy\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Homework 1",
    "section": "",
    "text": "I like to plot data\n\n\nI also analyze data"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Homework 1",
    "section": "",
    "text": "I also analyze data"
  },
  {
    "objectID": "index.html#sub-header",
    "href": "index.html#sub-header",
    "title": "Warren Kennedy",
    "section": "Sub-Header",
    "text": "Sub-Header"
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "",
    "text": "In this report, we present an analysis of various forecasting methods applied to a dataset containing the demand for eggs from Week 1 to Week 500. We compared four forecasting techniques: Oracle (theoretical maximum profit), Simple Exponential Smoothing (Simple ES), Holt’s Linear Trend Method (Holt), and a tuned version of Holt’s method (Holt tuned). Our goal was to determine which method provides the best forecast accuracy and profitability."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "",
    "text": "In this report, we present an analysis of various forecasting methods applied to a dataset containing the demand for eggs from Week 1 to Week 500. We compared four forecasting techniques: Oracle (theoretical maximum profit), Simple Exponential Smoothing (Simple ES), Holt’s Linear Trend Method (Holt), and a tuned version of Holt’s method (Holt tuned). Our goal was to determine which method provides the best forecast accuracy and profitability."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data The data used for this experiment has assigned roughly 2/3 of the individuals to the treatment group while assigning the remaining 1/3 of participants to be receive the standard mailing in the control group. Within the treatment group, we see that the different treatment features are evenly distributed across the treatment group participants\n\n# read the data \nlibrary(haven)\ndata &lt;- read_dta(\"/home/jovyan/My_Quarto_Website/projects/project1/karlan_list_2007.dta\")\n\n# summarize treatment/control split\ntreatment_perc &lt;- mean(data$treatment)\ncontrol_perc &lt;-mean(data$control)\n\n# Print the result\nprint(treatment_perc)\n\n[1] 0.6668131\n\nprint(control_perc)\n\n[1] 0.3331869\n\n\n\n# Create tables to show the distributions of the treatment features\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nratio_counts &lt;- data %&gt;%\n  count(ratio) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100,2))\n\nsize_counts &lt;- data %&gt;%\n  count(size) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100,2))\n\nask_counts &lt;- data %&gt;%\n  count(ask) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100,2))\n\n# Print the result\nprint(ratio_counts)\n\n# A tibble: 4 × 3\n  ratio           n percentage\n  &lt;dbl+lbl&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 0 [Control] 16687       33.3\n2 1           11133       22.2\n3 2           11134       22.2\n4 3           11129       22.2\n\nprint(ask_counts)\n\n# A tibble: 4 × 3\n  ask             n percentage\n  &lt;dbl+lbl&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 0 [Control] 16687       33.3\n2 1 [1x]      11134       22.2\n3 2 [1.25x]   11133       22.2\n4 3 [1.50x]   11129       22.2\n\nprint(size_counts)\n\n# A tibble: 5 × 3\n  size             n percentage\n  &lt;dbl+lbl&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 0 [Control]  16687       33.3\n2 1 [$25,000]   8350       16.7\n3 2 [$50,000]   8345       16.7\n4 3 [$100,000]  8350       16.7\n5 4 [Unstated]  8351       16.7\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nWe test for a difference in means using both the t-test method and separately the linear regression method. In this case, we use both methods to confirm what we are seeing, however, both methods will ultimately produce the same result. Comparing the means of both the couple and pwhite variables respectively, we see that there is no statistically significant difference between the observations in the treatment and control groups. We tested this hypothesis for each variable at a 95% confidence level. These results give us the confidence to conclude that we do not have the statistical evidence to reject the null hypothesis that there is no difference in the means between the two groups.\n\nComparing Means for couple\n\n#split data into treatment and control\ncontrol_df &lt;- data[data$treatment == 0, ]\ntreatment_df &lt;- data[data$treatment == 1, ]\n\n# create data for ttest\nmean_control &lt;- mean(control_df$couple, na.rm = TRUE)\nsd_control &lt;- sd(control_df$couple, na.rm = TRUE)\nN_control &lt;- length(control_df$couple)\n\nmean_treatment &lt;- mean(treatment_df$couple, na.rm = TRUE)\nsd_treatment &lt;- sd(treatment_df$couple, na.rm = TRUE)\nN_treatment &lt;- length(treatment_df$couple)\n  \n# calculate tstat\nt_stat &lt;- (mean_control - mean_treatment) / sqrt((sd_control^2 / N_control) + (sd_treatment^2 / N_treatment))\nprint(t_stat)\n\n[1] 0.5888429\n\n\n\ncouple_lm &lt;- lm(couple~treatment, data=data)\nsummary(couple_lm)\n\n\nCall:\nlm(formula = couple ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09297 -0.09297 -0.09136 -0.09136  0.90864 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.092975   0.002261  41.124   &lt;2e-16 ***\ntreatment   -0.001617   0.002770  -0.584    0.559    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2889 on 48933 degrees of freedom\n  (1148 observations deleted due to missingness)\nMultiple R-squared:  6.965e-06, Adjusted R-squared:  -1.347e-05 \nF-statistic: 0.3408 on 1 and 48933 DF,  p-value: 0.5594\n\n\n\n\nComparing Means for pwhite\n\n#split data into treatment and control\ncontrol_df &lt;- data[data$treatment == 0, ]\ntreatment_df &lt;- data[data$treatment == 1, ]\n\n# create data for ttest\nmean_control &lt;- mean(control_df$pwhite, na.rm = TRUE)\nsd_control &lt;- sd(control_df$pwhite, na.rm = TRUE)\nN_control &lt;- length(control_df$pwhite)\n\nmean_treatment &lt;- mean(treatment_df$pwhite, na.rm = TRUE)\nsd_treatment &lt;- sd(treatment_df$pwhite, na.rm = TRUE)\nN_treatment &lt;- length(treatment_df$pwhite)\n  \n# calculate tstat\nt_stat &lt;- (mean_control - mean_treatment) / sqrt((sd_control^2 / N_control) + (sd_treatment^2 / N_treatment))\nprint(t_stat)\n\n[1] 0.570052\n\n\n\n\nLinear Regression for pwhite\n\npwhite_lm &lt;- lm(pwhite~treatment, data=data)\nsummary(pwhite_lm)\n\n\nCall:\nlm(formula = pwhite ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81079 -0.06378  0.05300  0.11939  0.18070 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.8202078  0.0013309  616.28   &lt;2e-16 ***\ntreatment   -0.0009128  0.0016292   -0.56    0.575    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1686 on 48215 degrees of freedom\n  (1866 observations deleted due to missingness)\nMultiple R-squared:  6.51e-06,  Adjusted R-squared:  -1.423e-05 \nF-statistic: 0.3139 on 1 and 48215 DF,  p-value: 0.5753"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\nUsing a barplot to visually represen the outcome, we first observe the proportion of people who donated using one bar for treatment and one bar for control. Based on the bar plot below, there is a clear difference in the outcome: the treatement group has a higher proportion of people who donated than the control group. The barplot gives us reason to believe that the treatment may have a causal impact on the outcome of giving. Next, we test this hypothesis using statistical methods.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Calculating proportions\ndonation_summary &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(proportion_donated = mean(gave))\n\n# Ensure 'treatment' is a factor\ndonation_summary$treatment &lt;- as.factor(donation_summary$treatment)\n\n# Plotting the proportions\nggplot(donation_summary, aes(x = treatment, y = proportion_donated, fill = treatment)) +\n  geom_col() +  \n  scale_fill_manual(values = c(\"blue\", \"red\")) + \n  labs(title = \"Proportion of People Who Donated by Treatment Status\",\n       x = \"Treatment Status\",\n       y = \"Proportion that Donated\") +\n  theme_minimal()  \n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\nWe can test our hypothesis by using a t-test to compare the proportions observed between the treatment and control groups. As we have done before, we will confirm the results from our t-test using a bivariate linear regression to demonstrate the same finding.\nThe hypothesis we seek to test is whether or not the proportion of people who gave is the same in both the treatment and the control groups. The result from our t-test is that the treatment group is substantially different from the control group. A large t-stat(t-stat &gt; 2) tells us that we have evidence to suggest that the groups differ and in our case, the t-stat is greater than 3. The results from our bivariate linear regression confirm this finding, adding that being in the treatment group increases the probability of making a charitable donation by 0.004 percent. The third model we use to confirm our results is a probit model. The probit model confirms wht we see in both the t-test as well as the linear regression, allowing us to conclude that we have sufficient statistical evidence to conclude that the treatment has a positive effect on the likelihood of giving.\n\nT-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\n#split data into treatment and control\ncontrol_df &lt;- data[data$treatment == 0, ]\ntreatment_df &lt;- data[data$treatment == 1, ]\n\n# create data for ttest\nmean_control &lt;- mean(control_df$gave, na.rm = TRUE)\nsd_control &lt;- sd(control_df$gave, na.rm = TRUE)\nN_control &lt;- length(control_df$gave)\n\nmean_treatment &lt;- mean(treatment_df$gave, na.rm = TRUE)\nsd_treatment &lt;- sd(treatment_df$gave, na.rm = TRUE)\nN_treatment &lt;- length(treatment_df$gave)\n  \n# calculate tstat\nt_stat &lt;- (mean_control - mean_treatment) / sqrt((sd_control^2 / N_control) + (sd_treatment^2 / N_treatment))\nprint(t_stat)\n\n[1] -3.209462\n\n\n\n\nBi-Variate Linear Regression for whether any charitable donation was made.\n\ngave_lm &lt;- lm(gave~treatment, data=data)\nsummary(gave_lm)\n\n\nCall:\nlm(formula = gave ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n# Fit the probit regression model\nprobit &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\nsummary(probit)\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. Our results show that the match threshold does not have a statistically significant effect on whether or not people donate. Although this result goes against conventional wisdom, the finding aligns with what was observed in the study.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# Perform a t-test on outcome1\ntest12 &lt;- t.test(gave ~ ratio, data = subset(data, ratio %in% c(1, 2)))\ntest13 &lt;- t.test(gave ~ ratio, data = subset(data, ratio %in% c(1, 3)))\ntest23 &lt;- t.test(gave ~ ratio, data = subset(data, ratio %in% c(2, 3)))\n\n# Print results\nprint(test12)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 1 mean in group 2 \n     0.02074912      0.02263338 \n\nprint(test13)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 1 mean in group 3 \n     0.02074912      0.02273340 \n\nprint(test23)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 2 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 2 mean in group 3 \n     0.02263338      0.02273340 \n\n\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\nTo test the claim that the match threshold has no significant impact on whether or not someone decides to give, we execute a logistic regression, regressing gave on ratio1, ratio2, and ratio3. This test will tell us how each ratio effects the odds of someone giving anything. The coefficient for ratio 1 is positive but not statistically significant, meaning that the effect of the 1:1 ratio is not statistically significant from the effect observed when no match is offered. The effect of ratio’s 2 and 3 are both positive and statistically significant. Notably, their coefficients are nearly exactly the same, which shows tht the difference between a 2:1 ratio and a 3:1 ratio is essentially the same.\n\n# Create 'ratio1' \ndata &lt;- data %&gt;%\n  mutate(ratio1 = as.integer(ratio == 1))\n\n# Fit a logistic regression model\nmodel &lt;- glm(gave ~ ratio1 + ratio2 + ratio3, family = binomial(), data = data)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = gave ~ ratio1 + ratio2 + ratio3, family = binomial(), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.00727    0.05844 -68.565  &lt; 2e-16 ***\nratio1       0.15299    0.08852   1.728  0.08394 .  \nratio2       0.24184    0.08646   2.797  0.00516 ** \nratio3       0.24635    0.08637   2.852  0.00434 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10060  on 50079  degrees of freedom\nAIC: 10068\n\nNumber of Fisher Scoring iterations: 6\n\n\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n# create data subsets\nratio1_data &lt;- data[data$ratio == 1, ]\nratio2_data &lt;- data[data$ratio == 2, ]\nratio3_data &lt;- data[data$ratio == 3, ]\n\n# Calculate the mean of 'gave' for the subsets\nratio1_rr &lt;- mean(ratio1_data$gave, na.rm = TRUE) \nratio2_rr &lt;- mean(ratio2_data$gave, na.rm = TRUE) \nratio3_rr &lt;- mean(ratio3_data$gave, na.rm = TRUE) \n\nprint(ratio1_rr)\n\n[1] 0.02074912\n\nprint(ratio2_rr)\n\n[1] 0.02263338\n\nprint(ratio3_rr)\n\n[1] 0.0227334\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis? To measure the effect we run a bivariate linear regression of the donation amount on the treatment status. In the summary of this regression, we see that being in the treatment group has a positive effect on the size of the matched donation that is statistically significant at the 90 percent level.\n\namount_lm &lt;- lm(amount~treatment, data=data)\nsummary(amount_lm)\n\n\nCall:\nlm(formula = amount ~ treatment, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\nRepeating the analysis above using only the data from people who made a contribution, we are able to measure how much respondents donate conditional on donating some positive amount. In this case, we see that being in the treatment group has a negative impact, however, this impact is not statistically significant. This tells us that we are unsure whether the treatment is actually effecting the amount that people decide to give. Becuase our treatment coefficient is insignificant, we cannot conclude that the treatment has any effect on the amount that people decide to give. For this reason, we conclude that we do not have sufficient evidence to state that the treatment has a causal effect on the outcome.\n\ngivers_only &lt;- data[data$amount &gt; 0, ]\namount_lm &lt;- lm(amount~treatment, data=givers_only)\nsummary(amount_lm)\n\n\nCall:\nlm(formula = amount ~ treatment, data = givers_only)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\n# control group givers\ngivers_control &lt;- control_df[control_df$amount &gt; 0, ]\n\n# Calculate the mean of the donation amounts\nmean_amount &lt;- mean(givers_control$amount)\n\n# Creating the histogram with a vertical line indicating the mean\nhistogram1 &lt;- ggplot(givers_control, aes(x = amount)) +\n  geom_histogram(binwidth = 10, fill = \"blue\", color = \"black\") +  # Histogram\n  geom_vline(aes(xintercept = mean_amount), color = \"red\", linetype = \"dashed\", size = 1) +  # Mean line\n  labs(title = \"Donation Amounts (Control Group)\",\n       x = \"Donation Amount\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  ggtitle(paste(\"Control Group Donation Amounts with Mean at $\", round(mean_amount, 2)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Print the histogram\nprint(histogram1)\n\n\n\n\n\n\n\n\n\n# treatment group givers\ngivers_treatment &lt;- treatment_df[treatment_df$amount &gt; 0, ]\n\n# Calculate the mean of the donation amounts\nmean_amount &lt;- mean(givers_treatment$amount)\n\n# Creating the histogram with a vertical line indicating the mean\nhistogram1 &lt;- ggplot(givers_treatment, aes(x = amount)) +\n  geom_histogram(binwidth = 10, fill = \"blue\", color = \"black\") +  # Histogram\n  geom_vline(aes(xintercept = mean_amount), color = \"red\", linetype = \"dashed\", size = 1) +  # Mean line\n  labs(title = \"Donation Amounts (Treatment Group)\",\n       x = \"Donation Amount\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  ggtitle(paste(\"Treatment Group Donation Amounts with Mean at $\", round(mean_amount, 2)))\n\n# Print the histogram\nprint(histogram1)"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThis first plot is a demonstration of the Law of Large Numbers. This law states that as your sample size increases, you can expect your sample average to get closer to the true mean. To recreate this, we first simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. Next, we calculate a vector of 10,000 differences and then plot the cumulative average of that vector of differences. The plot shows that as we increase our sample size, the sample mean gets closer to the true mean. In our case, the sample mean never actually approaches the true difference in means. One simple way to fix this would be to increase our sample size beyond 10,000. to do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n# number of simulations\nn &lt;- 10000\n\n# probability of success\np_c &lt;- 0.018\np_t &lt;- 0.022\n\n# True mean difference\ntrue_diff &lt;- p_t - p_c\n\n# simulated draws\nset.seed(10)\ncontrol_sim &lt;- rbinom(n,size = 1, prob = p_c)\ntreatment_sim &lt;- rbinom(n,size = 1, prob = p_t)\n\n# Calculate cumulative means for both simulations\ncummean_control &lt;- cumsum(control_sim) / (1:n)\ncummean_treatment &lt;- cumsum(treatment_sim) / (1:n)\n\n# Calculate the cumulative differences\nsample_cumdiff &lt;- cummean_treatment - cummean_control\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Simulation = 1:n,\n  CumulativeDifference = sample_cumdiff\n)\n\n\n# Generate the plot\nggplot(plot_data, aes(x = Simulation, y = CumulativeDifference)) +\n  geom_line(color = \"red\") + \n  geom_hline(yintercept = true_diff, linetype = \"dashed\", color = \"blue\") + \n  labs(title = \"Cumulative Difference Between Treatment and Control\",\n       x = \"Number of Simulations\",\n       y = \"Cumulative Difference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.” To demonstrate the Central Limit Theorem we create histograms showing us the average difference between the treatment and control group from simulations of different sample sizes. The sample sizes we tested are 50, 200, 500, and 1000. As you can see in the plots, there are less observations in the tails of the data as the sample size increases. This observation validates the Central Limit Theorem because it shows that as we increase sample size the sample statistic gets closer to the true value. It is also worth noting that the value zero is in the middle of the distribution in each histogram.\n\n# Number of repetitions\nreps &lt;- 1000\n\n# Different numbers of draws\ndraw_sizes &lt;- c(50, 200, 500, 1000)\n\n# Probabilities for control and treatment groups\np_c &lt;- 0.018\np_t &lt;- 0.022\n\n# Set a seed for reproducibility\nset.seed(10)\n\n# Loop over each draw size\nfor (draws in draw_sizes) {\n    # Initialize a vector to store the average differences for the current draw size\n    mean_diff &lt;- numeric(reps)\n    \n    # Repeat draw and calculation for the current number of draws\n    for (i in 1:reps) {\n        # Simulate draws from Bernoulli distributions for control and treatment\n        c_sim &lt;- rbinom(draws, size = 1, prob = p_c)\n        t_sim &lt;- rbinom(draws, size = 1, prob = p_t)\n    \n        # Calculate the average difference and store it\n        mean_diff[i] &lt;- mean(t_sim) - mean(c_sim)\n    }\n    \n    # Visualize the distribution of average differences for the current draw size\n    plot_data &lt;- data.frame(AverageDifferences = mean_diff)\n    p &lt;- ggplot(plot_data, aes(x = AverageDifferences)) +\n        geom_histogram(bins = 30, fill = \"dodgerblue\", color = \"black\") +\n        labs(title = paste(\"Histogram of Average Differences for\", draws, \"Draws\"),\n             x = \"Average Difference\",\n             y = \"Frequency\") +\n        theme_minimal()\n    \n    print(p)  # Display the plot\n}"
  },
  {
    "objectID": "projects/project2/hw2_questions.html",
    "href": "projects/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo begin our analysis, we first need to read in the data.\n\n# read in the data\nblueprinty &lt;- read.csv(\"~/My_Quarto_Website/projects/project2/blueprinty.csv\")\n\nWe want to use this data to provide insights as to whether or not firms using Blueprinty’s software are more successful in getting their patent applications approved. We first visualize our data to see if there is an obvious difference in the number of patents approved between customers and non-customers. The plot below shows a roughly similar distribution between both groups. One notable difference is that, for customers, we see more volume centered around 4 while the non-customers’ number of patents approved seems to be equally likely to be between 0 and 4. We cannot be certain looking at the plots alone, so we continue by looking at the average number of patents approved given customer status.\n\ncustomers &lt;- blueprinty[blueprinty$iscustomer == 1, ]\nnoncustomers &lt;- blueprinty[blueprinty$iscustomer == 0, ]\n\n# Plot the first histogram\nhist(noncustomers$patents, \n     col = \"blue\", \n     main = \"Histogram Comparison\", \n     xlab = \"Number of Patents\", \n     ylab = \"Frequency\",\n     ylim = c(0, max(hist(noncustomers$patents)$counts, hist(customers$patents)$counts))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Add the second histogram to the same plot\nhist(customers$patents, \n     col = \"red\", \n     add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\", \n       legend = c(\"Non-Customers\", \"Customers\"), \n       fill = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nThe means for both groups are listed below. The mean for patents approved is roughly the same between customers and non-customers, but we can not be sure if there is a statistically significant difference between the two groups by simply comparing the means. To get more insights we can use linear regression to test the effect of being a customer.\n\n#split data into customer and non-customer\ncustomers &lt;- blueprinty[blueprinty$iscustomer == 1, ]\nnoncustomers &lt;- blueprinty[blueprinty$iscustomer == 0, ]\n\npatent_mean_c &lt;- mean(customers$patents) \npatent_mean_nc &lt;- mean(noncustomers$patents)\n\nprint(patent_mean_c)\n\n[1] 4.091371\n\nprint(patent_mean_nc)\n\n[1] 3.623177\n\n\nOur linear model estimates that there is indeed a positive causal effect from being a customer on the number of patents that are approved. This finding is statistically significant at the 95 percent level.\n\npatents_lm &lt;- lm(patents~iscustomer, data=blueprinty)\nsummary(patents_lm)\n\n\nCall:\nlm(formula = patents ~ iscustomer, data = blueprinty)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0914 -1.6232 -0.6232  1.3768 12.3768 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.62318    0.06505  55.702  &lt; 2e-16 ***\niscustomer   0.46819    0.17949   2.609  0.00918 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.348 on 1498 degrees of freedom\nMultiple R-squared:  0.004522,  Adjusted R-squared:  0.003857 \nF-statistic: 6.804 on 1 and 1498 DF,  p-value: 0.009184\n\n\nDespite our findings, it is important to note that the Blueprinty customers may not be selected at random. There may be systematic differences in the age and regional location of customers vs non-customers. To check whether or not there is a systematic difference in the composition of the group, we will look at the group level compositions for both customers and non-customers. In the plots below, we compare the distributions for age and region.\n\n\nLooking at the distribution of region for customers and nun-customers, we see a distribution that is mostly similar. Most of the members in the data are either located in the Northeast or the Southwest, with each of the three remaining region seemingly being equally proportioned among the remaining data. Given the similar distributions for the groups, we do not see evidence of a systematic difference between customers and non-customers when comparing regions.\n\n# Get frequency counts for each group in column_name1\ngroup_counts1 &lt;- table(noncustomers$region)\n\n# Plot a bar plot of the frequency counts for column_name1\nbarplot(group_counts1,\n        main = \"Region Frequency Comparison\",\n        xlab = \"Regions\",\n        ylab = \"Frequency\",\n        col = \"blue\",\n        border = \"black\"\n)\n\n# Get frequency counts for each group in column_name2\ngroup_counts2 &lt;- table(customers$region)\n\n# Add the bar plot of the frequency counts for column_name2 to the existing plot\nbarplot(group_counts2,\n        col = \"red\",\n        add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\",\n       legend = c(\"Non-Customer\", \"Customer\"),\n       fill = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\n\n\nWhen comparing the ages between the customer and non-customer groups we see a difference between the distribution of ages. The age data for customers is skewed right, while the data for non-customers is shaped more like a bell curve. This difference translates to the customer group being younger on average than the non-customer group. With this difference in mind, we have reason to believe that there may be a systematic difference between the customer and non-customer group that we will likely want to account for prior to drawing any conclusions.\n\n# Plot the first histogram\nhist(noncustomers$age, \n     col = \"blue\", \n     main = \"Histogram Comparison\", \n     xlab = \"Age\", \n     ylab = \"Frequency\",\n     ylim = c(0, max(hist(noncustomers$age)$counts, hist(customers$age)$counts))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Add the second histogram to the same plot\nhist(customers$age, \n     col = \"red\", \n     add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\", \n       legend = c(\"Non-Customers\", \"Customers\"), \n       fill = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density plot to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe mathematical likelihood for a Poisson distribution is as follows: \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nNext, we plot the Poisson likelihood (as well as the log-likelihood) as a function of using the number of patents observed as the input for Y.\n\npoisson_log_likelihood &lt;- function(lambda, Y){\n   log_likelihood &lt;- sum(Y * log(lambda) - lambda - log(factorial(Y)))\n   return(sum(log_likelihood))\n}\n\n\n# Use the patents column from the blueprinty dataset\nY &lt;- blueprinty$patents\n\n# Create a sequence of lambda values\nlambda_values &lt;- seq(0.1, 20, by = 0.1)  # Start from 0.1 to avoid lambda = 0\n\n# Calculate log-likelihood for each lambda value using the vector of patents\nlog_likelihood_values &lt;- sapply(lambda_values, function(lambda) poisson_log_likelihood(lambda, Y))\n\n# Plot log-likelihood\nplot(lambda_values, log_likelihood_values, type = \"l\", col = \"red\", xlab = \"Lambda\", ylab = \"Log-Likelihood\", main = \"Poisson Log-Likelihood\", ylim = range(log_likelihood_values, na.rm = TRUE))\n\n\n\n\n\n\n\n\nWe can find the maximum likelihood estimate (MLE) of ( \\(\\lambda\\)) for the Poisson distribution by optimizing the log-likelihood function using the optim() function. Here’s how we can do it:\n\n# Define the log-likelihood function for the Poisson distribution\npoisson_log_likelihood &lt;- function(lambda, Y){\n   -sum(dpois(Y, lambda, log = TRUE))\n}\n\n# Use the patents column from the blueprinty dataset\nY &lt;- blueprinty$patents\n\n# Use optim() to find the MLE\nmle_result &lt;- optim(par = 1, fn = poisson_log_likelihood, Y = Y, lower = 0.01, upper = 20, method = \"Brent\")\n\n# Extract the MLE\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Convert region to factor\nblueprinty$region &lt;- as.factor(blueprinty$region)\nblueprinty$region &lt;- relevel(blueprinty$region, ref = \"Midwest\")\n\n# Create covariate matrix X\nX &lt;- model.matrix(~ age + I(age^2) + iscustomer + region, data=blueprinty)\ny &lt;- blueprinty$patents\n\n# Update likelihood function for Poisson regression\npoisson_reg_ll &lt;- function(beta, X, y){\n   lambda &lt;- exp(X %*% beta)\n   log_likelihood &lt;- sum(dpois(y, lambda, log = TRUE))\n  return(log_likelihood)\n}\n\n\n## Use optim() to find the MLE vector and the Hessian\nout &lt;- optim(par = rep(0, ncol(X)), fn = poisson_reg_ll, X = X, y = y, hessian = TRUE, control = list(fnscale = -1))\n\n\n## Calculate standard errors of beta parameter estimates\nHinv &lt;- -solve(out$hessian)\nresults &lt;- cbind(coefs = out$par, sterr = sqrt(diag(Hinv)))\nrownames(results) &lt;- colnames(X)\nresults &lt;- round(results, 4)\n\n## Display coefficients and standard errors\nresults\n\n                  coefs  sterr\n(Intercept)     -0.4298 0.1145\nage              0.1449 0.0066\nI(age^2)        -0.0029 0.0001\niscustomer       0.1242 0.0389\nregionNortheast  0.0703 0.0416\nregionNorthwest -0.0285 0.0532\nregionSouth      0.0072 0.0527\nregionSouthwest  0.0139 0.0469\n\n\nWe can test the validity of our results using a generalized linear model.\n\n# Fit a Poisson regression model using glm\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, data = blueprinty, family = \"poisson\")\n\n# Display summary of the glm model\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = \"poisson\", data = blueprinty)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.4512823  0.1836301  -2.458  0.01399 *  \nage              0.1445364  0.0138793  10.414  &lt; 2e-16 ***\nI(age^2)        -0.0028681  0.0002577 -11.131  &lt; 2e-16 ***\nregionNortheast  0.0985960  0.0420070   2.347  0.01892 *  \nregionNorthwest -0.0200942  0.0537833  -0.374  0.70869    \nregionSouth      0.0571720  0.0526757   1.085  0.27776    \nregionSouthwest  0.0513470  0.0472124   1.088  0.27678    \niscustomer       0.1181144  0.0389204   3.035  0.00241 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2178.8  on 1492  degrees of freedom\nAIC: 6567.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterpreting the results of our regression we can cnofirm that being a customer has a positive and statistically significant impact on the likelihood of a patent application being approved. This finding supports the initial claim that firms using Blueprinty’s software are more successful in getting their patent applications approved. Our regression results are also telling us that age and region have a statistically significant effect on patent outcomes. This finding is consistent with the differences we had observed in patent outcomes and age/region distributions between customers and non customers. Given our results, it would be wise for a firm to consider becoming a customer to realize the effects of Blueprinty’s software on patent success."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo begin our analysis, we first need to read in the data.\n\n# read in the data\nblueprinty &lt;- read.csv(\"~/My_Quarto_Website/projects/project2/blueprinty.csv\")\n\nWe want to use this data to provide insights as to whether or not firms using Blueprinty’s software are more successful in getting their patent applications approved. We first visualize our data to see if there is an obvious difference in the number of patents approved between customers and non-customers. The plot below shows a roughly similar distribution between both groups. One notable difference is that, for customers, we see more volume centered around 4 while the non-customers’ number of patents approved seems to be equally likely to be between 0 and 4. We cannot be certain looking at the plots alone, so we continue by looking at the average number of patents approved given customer status.\n\ncustomers &lt;- blueprinty[blueprinty$iscustomer == 1, ]\nnoncustomers &lt;- blueprinty[blueprinty$iscustomer == 0, ]\n\n# Plot the first histogram\nhist(noncustomers$patents, \n     col = \"blue\", \n     main = \"Histogram Comparison\", \n     xlab = \"Number of Patents\", \n     ylab = \"Frequency\",\n     ylim = c(0, max(hist(noncustomers$patents)$counts, hist(customers$patents)$counts))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Add the second histogram to the same plot\nhist(customers$patents, \n     col = \"red\", \n     add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\", \n       legend = c(\"Non-Customers\", \"Customers\"), \n       fill = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\nThe means for both groups are listed below. The mean for patents approved is roughly the same between customers and non-customers, but we can not be sure if there is a statistically significant difference between the two groups by simply comparing the means. To get more insights we can use linear regression to test the effect of being a customer.\n\n#split data into customer and non-customer\ncustomers &lt;- blueprinty[blueprinty$iscustomer == 1, ]\nnoncustomers &lt;- blueprinty[blueprinty$iscustomer == 0, ]\n\npatent_mean_c &lt;- mean(customers$patents) \npatent_mean_nc &lt;- mean(noncustomers$patents)\n\nprint(patent_mean_c)\n\n[1] 4.091371\n\nprint(patent_mean_nc)\n\n[1] 3.623177\n\n\nOur linear model estimates that there is indeed a positive causal effect from being a customer on the number of patents that are approved. This finding is statistically significant at the 95 percent level.\n\npatents_lm &lt;- lm(patents~iscustomer, data=blueprinty)\nsummary(patents_lm)\n\n\nCall:\nlm(formula = patents ~ iscustomer, data = blueprinty)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0914 -1.6232 -0.6232  1.3768 12.3768 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.62318    0.06505  55.702  &lt; 2e-16 ***\niscustomer   0.46819    0.17949   2.609  0.00918 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.348 on 1498 degrees of freedom\nMultiple R-squared:  0.004522,  Adjusted R-squared:  0.003857 \nF-statistic: 6.804 on 1 and 1498 DF,  p-value: 0.009184\n\n\nDespite our findings, it is important to note that the Blueprinty customers may not be selected at random. There may be systematic differences in the age and regional location of customers vs non-customers. To check whether or not there is a systematic difference in the composition of the group, we will look at the group level compositions for both customers and non-customers. In the plots below, we compare the distributions for age and region.\n\n\nLooking at the distribution of region for customers and nun-customers, we see a distribution that is mostly similar. Most of the members in the data are either located in the Northeast or the Southwest, with each of the three remaining region seemingly being equally proportioned among the remaining data. Given the similar distributions for the groups, we do not see evidence of a systematic difference between customers and non-customers when comparing regions.\n\n# Get frequency counts for each group in column_name1\ngroup_counts1 &lt;- table(noncustomers$region)\n\n# Plot a bar plot of the frequency counts for column_name1\nbarplot(group_counts1,\n        main = \"Region Frequency Comparison\",\n        xlab = \"Regions\",\n        ylab = \"Frequency\",\n        col = \"blue\",\n        border = \"black\"\n)\n\n# Get frequency counts for each group in column_name2\ngroup_counts2 &lt;- table(customers$region)\n\n# Add the bar plot of the frequency counts for column_name2 to the existing plot\nbarplot(group_counts2,\n        col = \"red\",\n        add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\",\n       legend = c(\"Non-Customer\", \"Customer\"),\n       fill = c(\"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\n\n\nWhen comparing the ages between the customer and non-customer groups we see a difference between the distribution of ages. The age data for customers is skewed right, while the data for non-customers is shaped more like a bell curve. This difference translates to the customer group being younger on average than the non-customer group. With this difference in mind, we have reason to believe that there may be a systematic difference between the customer and non-customer group that we will likely want to account for prior to drawing any conclusions.\n\n# Plot the first histogram\nhist(noncustomers$age, \n     col = \"blue\", \n     main = \"Histogram Comparison\", \n     xlab = \"Age\", \n     ylab = \"Frequency\",\n     ylim = c(0, max(hist(noncustomers$age)$counts, hist(customers$age)$counts))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Add the second histogram to the same plot\nhist(customers$age, \n     col = \"red\", \n     add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\", \n       legend = c(\"Non-Customers\", \"Customers\"), \n       fill = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density plot to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe mathematical likelihood for a Poisson distribution is as follows: \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nNext, we plot the Poisson likelihood (as well as the log-likelihood) as a function of using the number of patents observed as the input for Y.\n\npoisson_log_likelihood &lt;- function(lambda, Y){\n   log_likelihood &lt;- sum(Y * log(lambda) - lambda - log(factorial(Y)))\n   return(sum(log_likelihood))\n}\n\n\n# Use the patents column from the blueprinty dataset\nY &lt;- blueprinty$patents\n\n# Create a sequence of lambda values\nlambda_values &lt;- seq(0.1, 20, by = 0.1)  # Start from 0.1 to avoid lambda = 0\n\n# Calculate log-likelihood for each lambda value using the vector of patents\nlog_likelihood_values &lt;- sapply(lambda_values, function(lambda) poisson_log_likelihood(lambda, Y))\n\n# Plot log-likelihood\nplot(lambda_values, log_likelihood_values, type = \"l\", col = \"red\", xlab = \"Lambda\", ylab = \"Log-Likelihood\", main = \"Poisson Log-Likelihood\", ylim = range(log_likelihood_values, na.rm = TRUE))\n\n\n\n\n\n\n\n\nWe can find the maximum likelihood estimate (MLE) of ( \\(\\lambda\\)) for the Poisson distribution by optimizing the log-likelihood function using the optim() function. Here’s how we can do it:\n\n# Define the log-likelihood function for the Poisson distribution\npoisson_log_likelihood &lt;- function(lambda, Y){\n   -sum(dpois(Y, lambda, log = TRUE))\n}\n\n# Use the patents column from the blueprinty dataset\nY &lt;- blueprinty$patents\n\n# Use optim() to find the MLE\nmle_result &lt;- optim(par = 1, fn = poisson_log_likelihood, Y = Y, lower = 0.01, upper = 20, method = \"Brent\")\n\n# Extract the MLE\nlambda_mle &lt;- mle_result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# Convert region to factor\nblueprinty$region &lt;- as.factor(blueprinty$region)\nblueprinty$region &lt;- relevel(blueprinty$region, ref = \"Midwest\")\n\n# Create covariate matrix X\nX &lt;- model.matrix(~ age + I(age^2) + iscustomer + region, data=blueprinty)\ny &lt;- blueprinty$patents\n\n# Update likelihood function for Poisson regression\npoisson_reg_ll &lt;- function(beta, X, y){\n   lambda &lt;- exp(X %*% beta)\n   log_likelihood &lt;- sum(dpois(y, lambda, log = TRUE))\n  return(log_likelihood)\n}\n\n\n## Use optim() to find the MLE vector and the Hessian\nout &lt;- optim(par = rep(0, ncol(X)), fn = poisson_reg_ll, X = X, y = y, hessian = TRUE, control = list(fnscale = -1))\n\n\n## Calculate standard errors of beta parameter estimates\nHinv &lt;- -solve(out$hessian)\nresults &lt;- cbind(coefs = out$par, sterr = sqrt(diag(Hinv)))\nrownames(results) &lt;- colnames(X)\nresults &lt;- round(results, 4)\n\n## Display coefficients and standard errors\nresults\n\n                  coefs  sterr\n(Intercept)     -0.4298 0.1145\nage              0.1449 0.0066\nI(age^2)        -0.0029 0.0001\niscustomer       0.1242 0.0389\nregionNortheast  0.0703 0.0416\nregionNorthwest -0.0285 0.0532\nregionSouth      0.0072 0.0527\nregionSouthwest  0.0139 0.0469\n\n\nWe can test the validity of our results using a generalized linear model.\n\n# Fit a Poisson regression model using glm\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, data = blueprinty, family = \"poisson\")\n\n# Display summary of the glm model\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = \"poisson\", data = blueprinty)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.4512823  0.1836301  -2.458  0.01399 *  \nage              0.1445364  0.0138793  10.414  &lt; 2e-16 ***\nI(age^2)        -0.0028681  0.0002577 -11.131  &lt; 2e-16 ***\nregionNortheast  0.0985960  0.0420070   2.347  0.01892 *  \nregionNorthwest -0.0200942  0.0537833  -0.374  0.70869    \nregionSouth      0.0571720  0.0526757   1.085  0.27776    \nregionSouthwest  0.0513470  0.0472124   1.088  0.27678    \niscustomer       0.1181144  0.0389204   3.035  0.00241 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2178.8  on 1492  degrees of freedom\nAIC: 6567.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterpreting the results of our regression we can cnofirm that being a customer has a positive and statistically significant impact on the likelihood of a patent application being approved. This finding supports the initial claim that firms using Blueprinty’s software are more successful in getting their patent applications approved. Our regression results are also telling us that age and region have a statistically significant effect on patent outcomes. This finding is consistent with the differences we had observed in patent outcomes and age/region distributions between customers and non customers. Given our results, it would be wise for a firm to consider becoming a customer to realize the effects of Blueprinty’s software on patent success."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#airbnb-case-study",
    "href": "projects/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData\nTo begin our analysis, we first need to read in the data. At first glance, our data has a large amount of ‘NA’ values. If the missing values were few and randomly distributed, we could choose to simply remove the rows with NA values. However in this case, we would lose a significant amount of data if we were to resolve the missing data in this way. Instead we have chosen to replace the missing data with the most common values from their respective columns.\n\n# read in the data\nairbnb &lt;- read.csv(\"~/My_Quarto_Website/projects/project2/airbnb.csv\")\n\n# Function to replace NA with mode\nna_to_mode &lt;- function(x) {\n  if (is.factor(x) | is.character(x)) {\n    mode_val &lt;- as.character(which.max(table(x)))\n    x[is.na(x)] &lt;- mode_val\n  } else {\n    mode_val &lt;- as.numeric(names(which.max(table(x))))\n    x[is.na(x)] &lt;- mode_val\n  }\n  return(x)\n}\n\n# Apply the function to each column\nairbnb_clean &lt;- as.data.frame(lapply(airbnb, na_to_mode))\n\n\nVisualizing the Response Variable\nFor this data we use number_of_reviews as a proxy for a room being booked. To begin our analysis, we will visualize our data to see if there are any notable trends. The first plot below shows the distribution for the number of reviews variables. As expected, our data is right-skewed, indicating that most rooms have a higher probability of having few reviews as opposed to many.\n\nlibrary(ggplot2)\nggplot(airbnb_clean, aes(x = airbnb_clean$number_of_reviews)) +\n  geom_histogram()\n\nWarning: Use of `airbnb_clean$number_of_reviews` is discouraged.\nℹ Use `number_of_reviews` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nmax(airbnb_clean$number_of_reviews)\n\n[1] 421\n\n\nThe second trend that we note is that the number_of_reviews is positively correlated with the number of days a unit has been listed. This trend is to be expected given that the higher the number of days that a unit is listed, the more time the unit has to be given a review.\n\n# Find the 10th percentile of the \"days\" variable\npercentile_10 &lt;- quantile(airbnb_clean$days, 0.1)\n\n# Subset the data to include only the lowest 90 percent of the \"days\" variable\nsubset_df &lt;- airbnb_clean[airbnb_clean$days &lt;= percentile_10, ]\n\nggplot(subset_df, aes(x = days, y = number_of_reviews)) +\n  geom_point() +  # Add points\n  geom_line()   \n\n\n\n\n\n\n\n\nNext we take a look at the difference between units that offer an instant booking option against those that don’t. The mean values for the two groups are different, so we will want to compare the overall composition of the groups to see if they are any adjustments that need to be made for our analysis.\n\n#split data into customer and non-customer\ninstant_book &lt;- airbnb_clean[airbnb_clean$instant_bookable == 't', ]\nno_instant_book &lt;- airbnb_clean[airbnb_clean$instant_bookable == 'f', ]\n\nreview_mean_ib &lt;- mean(instant_book$number_of_reviews) \nreview_mean_nonib &lt;- mean(no_instant_book$number_of_reviews)\n\nprint(review_mean_ib)\n\n[1] 20.89401\n\nprint(review_mean_nonib)\n\n[1] 14.70588\n\n\nIn the plot below, we see that the number of reviews can vary significantly depending on the room type. Entire homes and apartments tend to receive the most reviews while shared rooms hardly receive any. This difference in outcomes can be used to compare the distributions between Instant booking and non-instant booking units.\n\n# Create a bar plot\nggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  labs(x = \"Room Type\", y = \"Number of Reviews\", title = \"Number of Reviews by Room Type\")\n\n\n\n\n\n\n\n\nWhen we compare the room types between both groups, we see that they are relatively the same. We will continue our analysis assuming that the two groups have a similar composition.\n\n# Get frequency counts for each group in column_name1\ngroup_counts1 &lt;- table(no_instant_book$room_type)\n\n# Plot a bar plot of the frequency counts for column_name1\nbarplot(group_counts1,\n        main = \"Room_Type Comparison\",\n        xlab = \"Room Type\",\n        ylab = \"Frequency\",\n        col = \"dodgerblue\",\n        border = \"black\"\n)\n\n# Get frequency counts for each group in column_name2\ngroup_counts2 &lt;- table(instant_book$room_type)\n\n# Add the bar plot of the frequency counts for column_name2 to the existing plot\nbarplot(group_counts2,\n        col = \"red\",\n        add = TRUE\n)\n\n# Add a legend\nlegend(\"topright\",\n       legend = c(\"No Instant Book\", \"Instant Book\"),\n       fill = c(\"dodgerblue\", \"red\")\n)\n\n\n\n\n\n\n\n\n\n\n\nModeling the data\nThe Poisson regression model explores the factors influencing the number of reviews for Airbnb listings. The intercept (5.537) represents the expected number of reviews when all other variables are zero, which is not practically meaningful in this context. For each additional day of availability, the expected number of reviews increases by a factor of 1.00005021. Private rooms are associated with a 14.57% decrease, and shared rooms with a 41.71% decrease in the expected number of reviews compared to entire home/apartment listings. Each additional bathroom is linked to a 11.61% decrease, while each additional bedroom corresponds to a 7.02% increase in the expected number of reviews. Higher prices are associated with a slight decrease in reviews, with each unit increase leading to a 0.024% decrease. Higher review scores for cleanliness, location, and value are associated with increases in the expected number of reviews. Lastly, listings that are instant bookable have an expected 34.68% increase in reviews compared to non-instant bookable listings.\n\nmodel &lt;- glm(number_of_reviews ~ days + room_type + bathrooms + bedrooms + price + review_scores_cleanliness +review_scores_location + review_scores_value + instant_bookable, data = airbnb_clean , family = poisson)\n\nsummary(model)\n\n\nCall:\nglm(formula = number_of_reviews ~ days + room_type + bathrooms + \n    bedrooms + price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson, \n    data = airbnb_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                5.537e+00  1.289e-02  429.50   &lt;2e-16 ***\ndays                       5.021e-05  3.547e-07  141.56   &lt;2e-16 ***\nroom_typePrivate room     -1.457e-01  2.785e-03  -52.33   &lt;2e-16 ***\nroom_typeShared room      -4.171e-01  8.595e-03  -48.53   &lt;2e-16 ***\nbathrooms                 -1.161e-01  3.800e-03  -30.55   &lt;2e-16 ***\nbedrooms                   7.018e-02  1.990e-03   35.26   &lt;2e-16 ***\nprice                     -2.441e-04  1.082e-05  -22.55   &lt;2e-16 ***\nreview_scores_cleanliness  3.794e-02  1.454e-03   26.09   &lt;2e-16 ***\nreview_scores_location    -1.803e-01  1.541e-03 -116.99   &lt;2e-16 ***\nreview_scores_value       -1.480e-01  1.777e-03  -83.31   &lt;2e-16 ***\ninstant_bookablet          3.468e-01  2.875e-03  120.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1340709  on 40627  degrees of freedom\nResidual deviance: 1262681  on 40617  degrees of freedom\nAIC: 1386748\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we confirm our results using a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\).\n\n# Convert region to factor\nairbnb_clean$room_type &lt;- as.factor(airbnb_clean$room_type)\nairbnb_clean$room_type &lt;- relevel(airbnb_clean$room_type, ref = \"Shared room\")\n\n# Create covariate matrix X\nX2 &lt;- model.matrix(~ ., data=airbnb_clean)\ny2 &lt;- airbnb_clean$number_of_reviews\n\n# Update likelihood function for Poisson regression\npoisson_reg_ll &lt;- function(beta, X2, y2){\n   lambda &lt;- exp(X2 %*% beta)\n   log_likelihood &lt;- sum(dpois(y2, lambda, log = TRUE))\n  return(log_likelihood)\n}\n\n\n## Use optim() to find the MLE vector and the Hessian\n#out &lt;- optim(par = rep(0, ncol(X2)), fn = poisson_reg_ll, X = X2, y = y2, hessian = TRUE, control = list(fnscale = -1), method = 'BFGS')\n\n\n## Calculate standard errors of beta parameter estimates\n#Hinv &lt;- -solve(out$hessian)\n#results &lt;- cbind(coefs = out$par, sterr = sqrt(diag(Hinv)))\n#rownames(results) &lt;- colnames(X2)\n#results &lt;- round(results, 4)\n\n## Display coefficients and standard errors\n#results"
  },
  {
    "objectID": "projects/project3/hw3_questions.html",
    "href": "projects/project3/hw3_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "This assignment uses uses the Multi-nomial Logit model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/project3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Poisson Regression Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, maybe show the first few rows, and describe the data a bit.\nTo begin our analysis, we examine the data set to identify any noticeable trends. The bar plot below illustrates several key observations.\nFirst, in terms of product selection frequency, each product demonstrates distinct levels of popularity. Product 2 was chosen most frequently, with 975 selections, followed by product 1 with 831 selections, product 4 with 553 selections, and notably, product 3 with only 71 selections.\nSecond, regarding product the frequency of each product being featured, products 2, 3, and 4 were featured approximately 90 times each, while product 1 stood out with 135 features. These initial observations provide valuable insights into the relative popularity and feature exposure of each product, setting the stage for further in-depth analysis.\n\nyogurt_data &lt;- read.csv(\"~/My_Quarto_Website/projects/project3/yogurt_data.csv\", header=TRUE)\nhead(yogurt_data)\n\n  id y1 y2 y3 y4 f1 f2 f3 f4    p1    p2    p3    p4\n1  1  0  0  0  1  0  0  0  0 0.108 0.081 0.061 0.079\n2  2  0  1  0  0  0  0  0  0 0.108 0.098 0.064 0.075\n3  3  0  1  0  0  0  0  0  0 0.108 0.098 0.061 0.086\n4  4  0  1  0  0  0  0  0  0 0.108 0.098 0.061 0.086\n5  5  0  1  0  0  0  0  0  0 0.125 0.098 0.049 0.079\n6  6  0  1  0  0  0  0  0  0 0.108 0.092 0.050 0.079\n\n# frequency of product being chosen\nP1_chosen &lt;- sum(yogurt_data$y1 == 1)\nP2_chosen &lt;- sum(yogurt_data$y2 == 1)\nP3_chosen &lt;- sum(yogurt_data$y3 == 1)\nP4_chosen &lt;- sum(yogurt_data$y4 == 1)\n\nchoice_counts &lt;- c(P1_chosen, P2_chosen, P3_chosen, P4_chosen)\n\n# frequency of product being featured\nP1_featured &lt;- sum(yogurt_data$f1 == 1)\nP2_featured &lt;- sum(yogurt_data$f2 == 1)\nP3_featured &lt;- sum(yogurt_data$f3 == 1)\nP4_featured &lt;- sum(yogurt_data$f4 == 1)\n\nfeature_counts &lt;- c(P1_featured, P2_featured, P3_featured, P4_featured)\n\n\n# Plotting choice and feature frequency\npar(mfrow=c(1,1))\ncombined_counts &lt;- rbind(choice_counts, feature_counts)\nbarplot(combined_counts, beside=TRUE, names.arg=c(\"Product 1\", \"Product 2\", \"Product 3\", \"Product 4\"),\n        xlab=\"Product\", ylab=\"Frequency\", main=\"Distribution of Products Chosen and Featured\",\n        col=c(\"blue\", \"red\"), legend=c(\"Chosen\", \"Featured\"))\n\n\n\n\n\n\n\n\nLastly, we observe the distribution of price per ounce charged for each product. The frequency plots for each product reveal distinct pricing patterns. Products 1, 2, and 4 exhibit a narrow distribution, with a common price point that dominates their sales. In contrast, product 3 displays a broader distribution, indicating multiple price points at which it is frequently sold. Despite this variability, product 3 was sold less frequently compared to the other products.\n\n# Load the dplyr package\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Assuming 'yogurt_data' is the name of your dataframe\n# Create a copy of the dataframe\nyogurt_data_copy &lt;- data.frame(yogurt_data)\n\n# Convert to numeric\nyogurt_data_copy$p1 &lt;- as.numeric(yogurt_data_copy$p1)\nyogurt_data_copy$p2 &lt;- as.numeric(yogurt_data_copy$p2)\nyogurt_data_copy$p3 &lt;- as.numeric(yogurt_data_copy$p3)\nyogurt_data_copy$p4 &lt;- as.numeric(yogurt_data_copy$p4)\n\n# Plot a histogram\nhist(yogurt_data_copy$p1, main = \"Histogram of Price for Product 1\", xlab = \"Price\", ylab = \"Frequency\")\n\n\n\n\n\n\n\nhist(yogurt_data_copy$p2, main = \"Histogram of Price for Product 2\", xlab = \"Price\", ylab = \"Frequency\")\n\n\n\n\n\n\n\nhist(yogurt_data_copy$p3, main = \"Histogram of Price for Product 3\", xlab = \"Price\", ylab = \"Frequency\")\n\n\n\n\n\n\n\nhist(yogurt_data_copy$p4, main = \"Histogram of Price for Product 4\", xlab = \"Price\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\ntodo: reshape and prep the data\n\ncol_names1 &lt;- c(\"y1\",\"y2\",\"y3\",\"f1\", \"p1\")\ncol_names2 &lt;- c(\"y1\",\"y2\",\"y3\",\"f2\", \"p2\")\ncol_names3 &lt;- c(\"y1\",\"y2\",\"y3\",\"f3\", \"p3\")\n\ny1 &lt;- yogurt_data[,col_names1, drop=FALSE]\ny2 &lt;- yogurt_data[,col_names2, drop=FALSE]\ny3 &lt;- yogurt_data[,col_names3, drop=FALSE]\n\nnames(y1) &lt;- c(\"y1\",\"y2\",\"y3\",\"featured\",\"price\")\nnames(y2) &lt;- c(\"y1\",\"y2\",\"y3\",\"featured\",\"price\")\nnames(y3) &lt;- c(\"y1\",\"y2\",\"y3\",\"featured\",\"price\")\n\ny1$Alternative &lt;- \"1\"\ny2$Alternative &lt;- \"2\"\ny3$Alternative &lt;- \"3\"\n\ny1$choice &lt;- ifelse(y1$y1==1,1,0)\ny2$choice &lt;- ifelse(y2$y2==1,1,0)\ny3$choice &lt;- ifelse(y3$y3==1,1,0)\n\nyogurt_long &lt;- rbind(y1,y2,y3)\n\n # Assuming your dataframe is called data_df\nyogurt_long$chosen_alternative &lt;- ifelse(yogurt_long$y1 == 1, 1,\n                                         ifelse(yogurt_long$y2 == 2, 2,\n                                                ifelse(yogurt_long$y3 == 3, 3, 4)))\n\n\n\nEstimation\ntodo: Code up the log-likelihood function.\n\nmnl_ll &lt;- function(beta, X) {\n  # Calculate the utility for each alternative\n  utilities &lt;- X %*% beta\n  \n  # Calculate the log of the sum of exponentials of utilities\n  log_sum_exp_utilities &lt;- log(sum(exp(utilities)))\n  \n  # Calculate the log-likelihood\n  ll &lt;- sum(utilities) - length(utilities) * log_sum_exp_utilities\n  \n  return(-ll)  # Return negative log-likelihood for minimization\n}\n\ntodo: Use optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)). (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)\n\n# Create covariate matrix X\nX &lt;- model.matrix(~-1+ y1 + y2 + y3 + featured + price, data=yogurt_long)\ny &lt;- yogurt_long$choice\n\nout &lt;- optim(par=rep(0, ncol(X)), fn=mnl_ll, X=X, control = list(fnscale=-1))\nout$par\n\n[1] -284.88167  696.88937  123.61645   12.19981 -542.86766\n\n\n\n\nDiscussion\ntodo: interpret the 3 product intercepts (which yogurt is most preferred?). We learn based on the parameter estimates from the multinomial logit (MNL) model above that yogurt product 2 (y2) is the most preferred among the three options (y1, y2, y3). This is suggested by the positive coefficient estimate for y2 (\\((\\beta_2 \\approx 696.89\\))), indicating that an increase in the characteristics associated with y2 leads to a greater utility compared to y1 and y3. Additionally, the positive coefficient estimate for the featured variable (\\((\\beta_f \\approx 12.20\\))) suggests that featuring a yogurt product will further increase its utility to the consumer, although to a lesser extent compared to the characteristics of the yogurts themselves. On the other hand, the negative coefficient estimate for the price variable (\\((\\beta_p \\approx -542.87\\))) indicates that as the price of a yogurt increases, its utility decreases, which is consistent with expectations. Overall, based on these estimates, it can be inferred that consumers in this model prefer yogurt y2 over y1 and y3, and that featuring a yogurt positively influences its utility, while higher prices have a negative impact.\ntodo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\ntodo: calculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities. Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/project3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Poisson Regression Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\ntodo: download the dataset from here: http://goo.gl/5xQObB\n\nminivan &lt;- read.csv(\"~/My_Quarto_Website/projects/project3/rintro-chapter13conjoint.csv\",\n                    colClasses = c(seat = \"factor\", price = \"factor\", choice = \"integer\", eng = \"factor\", carpool = \"factor\", cargo = \"factor\"))\n\ntodo: describe the data a bit. How many respondents took the conjoint survey? How many choice tasks did each respondent complete? How many alternatives were presented on each choice task? For each alternative. Before fitting our choice model, we first describe the data to better understand what is going on. In the summary below, we see that 200 respondents took the conjoint survey. Each respondent completed 15 choice tasks, with each choice task consisting of 3 alternatives for the respondent to choose from. The alternatives consisted of varying levels of the minivan’s product attributes. The three levels of the seat,eng(ine), andprice product attributes appeared roughly 3000 times for each level, while the two levels for the cargo attribute appeared roughly 4500 times.\n\nsummary(minivan)\n\n    resp.id            ques         alt    carpool    seat     cargo     \n Min.   :  1.00   Min.   : 1   Min.   :1   no :6345   6:3024   2ft:4501  \n 1st Qu.: 50.75   1st Qu.: 4   1st Qu.:1   yes:2655   7:2993   3ft:4499  \n Median :100.50   Median : 8   Median :2              8:2983             \n Mean   :100.50   Mean   : 8   Mean   :2                                 \n 3rd Qu.:150.25   3rd Qu.:12   3rd Qu.:3                                 \n Max.   :200.00   Max.   :15   Max.   :3                                 \n   eng       price         choice      \n elec:3010   30:2998   Min.   :0.0000  \n gas :3005   35:2997   1st Qu.:0.0000  \n hyb :2985   40:3005   Median :0.0000  \n                       Mean   :0.3333  \n                       3rd Qu.:1.0000  \n                       Max.   :1.0000  \n\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\nMost popular attribute levels: - seat = 6 seats - eng = gas powered - price = $30,000 - cargo = 3ft of cargo\n\nxtabs(choice~seat, data = minivan)\n\nseat\n   6    7    8 \n1164  854  982 \n\nxtabs(choice~eng, data = minivan)\n\neng\nelec  gas  hyb \n 608 1444  948 \n\nxtabs(choice~price, data = minivan)\n\nprice\n  30   35   40 \n1486  956  558 \n\nxtabs(choice~cargo, data = minivan)\n\ncargo\n 2ft  3ft \n1312 1688 \n\n\n\n\nModel\ntodo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors. You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation.\n\nminivan2 &lt;- read.csv(\"~/My_Quarto_Website/projects/project3/rintro-chapter13conjoint.csv\",\n                    colClasses = c(seat = \"factor\", choice = \"integer\", eng = \"factor\", carpool = \"factor\", cargo = \"factor\", alt =\"factor\"))\n\n# Reorder levels to drop 6 seats, 2ft cargo, and gas engine levels\nminivan$seat &lt;- relevel(minivan$seat, ref = \"6\")\nminivan$cargo &lt;- relevel(minivan$cargo, ref = \"2ft\")\nminivan$eng &lt;- relevel(minivan$eng, ref = \"gas\")\n\n# Convert price to numeric (continuous variable)\nminivan$price &lt;- as.numeric(as.character(minivan$price))\n\n# One-hot encode specific columns\ncolumns &lt;- c(\"resp.id\", \"ques\", \"alt\", \"carpool\", \"seat\", \"eng\",\"price\", \"choice\")\nminivan2 &lt;- model.matrix(~ . - 1, data = minivan[,columns])\n\n# Convert minivan2 to a data frame\nminivan2_df &lt;- as.data.frame(minivan2)\n\n# Create covariate matrix X\nX2 &lt;- model.matrix(~-1+ carpoolyes + seat7 + seat8 + engelec + enghyb + price, data=minivan2_df)\ny2 &lt;- minivan2_df$choice\n\n# Optimize the log-likelihood function\nout &lt;- optim(par = rep(1, ncol(X2)), fn = mnl_ll, X = X2, control = list(fnscale = -1), hessian = TRUE)\n\n# Calculate standard errors\ncov_matrix &lt;- solve(-out$hessian)\nstandard_errors &lt;- sqrt(diag(cov_matrix))\n\nWarning in sqrt(diag(cov_matrix)): NaNs produced\n\n# Create a table of parameter values with beta's and standard errors\nparam_table &lt;- data.frame(\n  parameter = colnames(X2),\n  beta = out$par,\n  se = standard_errors\n)\n\n# Print the parameter table with standard errors\nprint(param_table)\n\n   parameter       beta  se\n1 carpoolyes  2718.4289 NaN\n2      seat7 -1243.7731 NaN\n3      seat8 -1457.5052   0\n4    engelec   243.5506   0\n5     enghyb  1409.2931 NaN\n6      price  -162.4070   0\n\n\n\n\nResults\ntodo: Interpret the coefficients. Which features are more preferred? Using the parameters from our model, we see that consumers most prefer minians with 6 seats and a hybrid engine.\ntodo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#methodology",
    "href": "projects/project1/hw1_questions.html#methodology",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Methodology",
    "text": "Methodology\nThe Oracle method represents the theoretical maximum possible profit, assuming perfect knowledge of future demand. It serves as a benchmark for comparing other forecasting methods. The profit calculation for the Oracle method uses the actual demand directly, thus providing the upper bound for potential profitability.\nSimple Exponential Smoothing (Simple ES) is a basic forecasting method that uses a smoothing parameter (alpha) to weight the most recent observations. We initially set alpha to 0.2 and calculated forecasts for the dataset. This method helps to smooth out short-term fluctuations and highlight longer-term trends or cycles. Additionally, we calculated the profit using a safety stock based on the forecast errors, ensuring that inventory levels could meet unexpected demand spikes.\nHolt’s Linear Trend Method (Holt) extends Simple ES by incorporating a trend component, making it suitable for data with a trend. Initially, we used default values for the smoothing parameters (alpha = 0.2, beta = 0.2). This method accounts for both the level of the series and the trend, thus providing a more nuanced forecast than Simple ES. We calculated the forecasts and profits in a manner similar to Simple ES, but with the added benefit of trend adjustment.\nTo improve the forecast accuracy of Holt’s method, we optimized the smoothing parameters (alpha and beta) using a grid search approach to minimize the Mean Squared Error (MSE). By tuning these parameters, we aimed to enhance the method’s responsiveness to changes in the data and reduce forecast errors. Using these optimized parameters, we recalculated the forecasts and profits, labeling this version as Holt tuned."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#results",
    "href": "projects/project1/hw1_questions.html#results",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Results",
    "text": "Results\nWe compared the average weekly profit from Week 301 to Week 500 for each method. The Oracle method, as expected, provided the highest possible profit since it is based on perfect foresight. Simple ES initially performed well, offering better forecasts than Holt’s default settings due to its simplicity and effectiveness in smoothing random variations in the data. However, when we tuned the parameters of Holt’s method, it showed a significant improvement in forecast accuracy and profitability, ultimately outperforming Simple ES.\nHolt’s default settings initially did not provide as high a profit as Simple ES, but after parameter optimization, Holt tuned demonstrated superior performance. This highlights the importance of fine-tuning forecasting models to better capture the underlying patterns in the data."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#conclusion",
    "href": "projects/project1/hw1_questions.html#conclusion",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Conclusion",
    "text": "Conclusion\nThe analysis demonstrated that while Simple Exponential Smoothing performed well initially, tuning the parameters of Holt’s Linear Trend Method resulted in superior forecasts and higher profitability. This underscores the importance of parameter optimization in forecasting models to achieve the best possible performance.\nBy leveraging these findings, organizations can enhance their demand forecasting accuracy and optimize inventory management, leading to improved financial outcomes. Accurate forecasts help in maintaining optimal inventory levels, reducing holding costs, and minimizing stockouts, thereby contributing to overall operational efficiency and customer satisfaction. This study illustrates the practical benefits of applying sophisticated forecasting techniques and the value of continuous improvement through model tuning."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#question-1",
    "href": "projects/project1/hw1_questions.html#question-1",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Question 1:",
    "text": "Question 1:\nUse the data from Week1 to Week300 to estimate the slope “a” in time (t) and the intercept “b” from the linear regression model.\n\n# Filter the relevant columns and rows\ntrain &lt;- df[1:300,]\n\n# Perform linear regression\nmodel &lt;- lm(demand ~ t, data = train)\n\n# Get the slope (a) and intercept (b)\na &lt;- coef(model)[\"t\"]\nb &lt;- coef(model)[\"(Intercept)\"]\n\n# Print the results\ncat(\"Slope (a):\", a, \"\\n\")\n\nSlope (a): 4.519219 \n\ncat(\"Intercept (b):\", b, \"\\n\")\n\nIntercept (b): 1919.371"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#question-2",
    "href": "projects/project1/hw1_questions.html#question-2",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Question 2:",
    "text": "Question 2:\nUse L0=b and T0=a from the above linear regression. Use alpha=0.2 and beta=0.2. Estimate the forecast from Week1 to Week 500.\n\n# Initialize parameters\nL0 &lt;- b\nT0 &lt;- a\nalpha &lt;- 0.2\nbeta &lt;- 0.2\n\n# Initialize level, trend, and forecast\nLt &lt;- c()\nTt &lt;- c()\nFt &lt;- c()\n\n# Holt-Winters forecasting\nfor (i in 1:500) {\n  if(i == 1) {\n    Lt[i] &lt;- alpha * df$demand[i] + (1 - alpha) * (L0 + T0)\n    Tt[i] &lt;- beta * (Lt[i] - L0) + (1 - beta) * T0\n    Ft[i] &lt;- L0 + T0\n  } else if (i &lt;= nrow(df)) {\n    Lt[i] &lt;- alpha * df$demand[i] + (1 - alpha) * (Lt[i-1] + Tt[i-1])\n    Tt[i] &lt;- beta * (Lt[i] - Lt[i-1]) + (1 - beta) * Tt[i-1]\n    Ft[i] &lt;- Lt[i-1] + Tt[i-1]\n  } else {\n    Lt[i] &lt;- Lt[i-1] + Tt[i-1]\n    Tt[i] &lt;- Tt[i-1]\n    Ft[i] &lt;- Lt[i-1] + Tt[i-1]\n  }\n}\n\n# Create a data frame to store the results\nforecast_df &lt;- data.frame(Week = 1:500, Forecast = Ft[1:500])\n\n# Print the forecast data\nprint(forecast_df)\n\n    Week Forecast\n1      1 1923.890\n2      2 1940.436\n3      3 1941.815\n4      4 1896.885\n5      5 1839.186\n6      6 1840.660\n7      7 1957.772\n8      8 1884.791\n9      9 2022.135\n10    10 2077.844\n11    11 2214.098\n12    12 2167.937\n13    13 2242.491\n14    14 2135.034\n15    15 2121.908\n16    16 2151.490\n17    17 2090.096\n18    18 1996.618\n19    19 1966.890\n20    20 2081.872\n21    21 2091.343\n22    22 2015.426\n23    23 2078.635\n24    24 2050.777\n25    25 2049.340\n26    26 2100.616\n27    27 1990.733\n28    28 1938.837\n29    29 2027.206\n30    30 2113.454\n31    31 2229.074\n32    32 2344.047\n33    33 2321.183\n34    34 2253.685\n35    35 2127.979\n36    36 2050.735\n37    37 1957.311\n38    38 1964.839\n39    39 1893.267\n40    40 1956.360\n41    41 1870.739\n42    42 1907.933\n43    43 1949.491\n44    44 2026.638\n45    45 2058.570\n46    46 2153.532\n47    47 2187.641\n48    48 2290.943\n49    49 2289.146\n50    50 2232.463\n51    51 2182.458\n52    52 2211.436\n53    53 2292.440\n54    54 2293.347\n55    55 2193.338\n56    56 2079.117\n57    57 2013.336\n58    58 2007.097\n59    59 1949.783\n60    60 2048.540\n61    61 2067.484\n62    62 2147.860\n63    63 2091.926\n64    64 2056.622\n65    65 1992.354\n66    66 2145.325\n67    67 2273.449\n68    68 2415.131\n69    69 2531.830\n70    70 2459.357\n71    71 2491.004\n72    72 2440.922\n73    73 2340.259\n74    74 2252.038\n75    75 2244.540\n76    76 2143.480\n77    77 2036.933\n78    78 2031.218\n79    79 1956.037\n80    80 2030.091\n81    81 1977.410\n82    82 1916.889\n83    83 2005.477\n84    84 2078.368\n85    85 2081.266\n86    86 2222.854\n87    87 2226.530\n88    88 2256.210\n89    89 2282.746\n90    90 2317.664\n91    91 2422.412\n92    92 2404.834\n93    93 2428.659\n94    94 2479.452\n95    95 2492.988\n96    96 2518.898\n97    97 2608.389\n98    98 2691.127\n99    99 2648.552\n100  100 2571.430\n101  101 2430.196\n102  102 2336.720\n103  103 2216.671\n104  104 2314.484\n105  105 2358.556\n106  106 2312.231\n107  107 2316.482\n108  108 2306.983\n109  109 2382.865\n110  110 2412.536\n111  111 2501.611\n112  112 2605.167\n113  113 2667.085\n114  114 2755.096\n115  115 2689.661\n116  116 2671.966\n117  117 2614.532\n118  118 2613.243\n119  119 2544.963\n120  120 2492.380\n121  121 2405.058\n122  122 2293.558\n123  123 2329.216\n124  124 2228.094\n125  125 2363.152\n126  126 2282.793\n127  127 2211.074\n128  128 2171.136\n129  129 2128.900\n130  130 2204.075\n131  131 2381.612\n132  132 2382.177\n133  133 2436.102\n134  134 2548.078\n135  135 2598.736\n136  136 2671.873\n137  137 2579.427\n138  138 2520.973\n139  139 2610.132\n140  140 2644.853\n141  141 2669.836\n142  142 2777.109\n143  143 2857.403\n144  144 2766.382\n145  145 2681.990\n146  146 2609.437\n147  147 2551.297\n148  148 2511.053\n149  149 2452.056\n150  150 2552.976\n151  151 2578.033\n152  152 2642.997\n153  153 2657.928\n154  154 2600.436\n155  155 2646.585\n156  156 2763.561\n157  157 2729.599\n158  158 2823.926\n159  159 2747.430\n160  160 2634.296\n161  161 2609.177\n162  162 2492.435\n163  163 2504.344\n164  164 2632.857\n165  165 2634.274\n166  166 2609.236\n167  167 2601.716\n168  168 2555.512\n169  169 2567.448\n170  170 2669.738\n171  171 2665.782\n172  172 2702.185\n173  173 2779.300\n174  174 2852.340\n175  175 2762.399\n176  176 2839.270\n177  177 2890.276\n178  178 2990.869\n179  179 2914.389\n180  180 2968.430\n181  181 2829.605\n182  182 2800.321\n183  183 2662.081\n184  184 2666.286\n185  185 2716.158\n186  186 2756.090\n187  187 2673.191\n188  188 2733.545\n189  189 2843.206\n190  190 2829.407\n191  191 2705.271\n192  192 2692.352\n193  193 2756.722\n194  194 2833.229\n195  195 2866.786\n196  196 2750.920\n197  197 2658.991\n198  198 2773.167\n199  199 2837.382\n200  200 2933.658\n201  201 2919.213\n202  202 2840.888\n203  203 2915.473\n204  204 2816.002\n205  205 2913.745\n206  206 2977.830\n207  207 2926.544\n208  208 2864.854\n209  209 2886.748\n210  210 2837.593\n211  211 2905.525\n212  212 2780.730\n213  213 2727.025\n214  214 2622.339\n215  215 2725.138\n216  216 2663.451\n217  217 2587.563\n218  218 2714.191\n219  219 2775.605\n220  220 2869.912\n221  221 3008.961\n222  222 3132.082\n223  223 3035.816\n224  224 2954.290\n225  225 2924.698\n226  226 2831.836\n227  227 2719.153\n228  228 2741.761\n229  229 2671.977\n230  230 2729.230\n231  231 2807.424\n232  232 2856.561\n233  233 2908.129\n234  234 2888.138\n235  235 2808.580\n236  236 2766.030\n237  237 2809.949\n238  238 2951.607\n239  239 2986.708\n240  240 2983.881\n241  241 2953.944\n242  242 2863.397\n243  243 2770.623\n244  244 2864.619\n245  245 2881.151\n246  246 2996.251\n247  247 3123.721\n248  248 3166.867\n249  249 3223.470\n250  250 3301.534\n251  251 3185.643\n252  252 3238.705\n253  253 3155.046\n254  254 3211.397\n255  255 3123.342\n256  256 3200.125\n257  257 3184.386\n258  258 3045.699\n259  259 3103.922\n260  260 3180.903\n261  261 3164.692\n262  262 3050.975\n263  263 3048.003\n264  264 3084.945\n265  265 3142.701\n266  266 3201.798\n267  267 3219.363\n268  268 3144.881\n269  269 3136.940\n270  270 3029.709\n271  271 3140.777\n272  272 3216.360\n273  273 3293.371\n274  274 3206.966\n275  275 3134.683\n276  276 3210.109\n277  277 3295.246\n278  278 3206.386\n279  279 3143.242\n280  280 3201.037\n281  281 3264.232\n282  282 3333.938\n283  283 3362.386\n284  284 3327.009\n285  285 3352.547\n286  286 3247.275\n287  287 3244.607\n288  288 3120.288\n289  289 3197.821\n290  290 3106.455\n291  291 3094.304\n292  292 3167.971\n293  293 3196.586\n294  294 3256.694\n295  295 3243.553\n296  296 3237.698\n297  297 3273.826\n298  298 3355.695\n299  299 3452.203\n300  300 3337.321\n301  301 3420.243\n302  302 3277.090\n303  303 3192.405\n304  304 3282.880\n305  305 3355.225\n306  306 3310.052\n307  307 3323.152\n308  308 3345.985\n309  309 3393.853\n310  310 3344.873\n311  311 3222.534\n312  312 3263.161\n313  313 3174.377\n314  314 3285.894\n315  315 3360.152\n316  316 3329.952\n317  317 3407.795\n318  318 3377.437\n319  319 3445.933\n320  320 3349.132\n321  321 3291.687\n322  322 3252.343\n323  323 3185.774\n324  324 3194.648\n325  325 3200.481\n326  326 3175.248\n327  327 3309.452\n328  328 3200.757\n329  329 3279.691\n330  330 3247.531\n331  331 3276.501\n332  332 3228.497\n333  333 3198.554\n334  334 3216.778\n335  335 3158.165\n336  336 3100.069\n337  337 3208.589\n338  338 3334.261\n339  339 3260.789\n340  340 3379.219\n341  341 3499.315\n342  342 3448.139\n343  343 3563.432\n344  344 3495.690\n345  345 3373.668\n346  346 3436.904\n347  347 3413.257\n348  348 3466.169\n349  349 3572.212\n350  350 3632.997\n351  351 3528.266\n352  352 3597.790\n353  353 3546.058\n354  354 3489.030\n355  355 3364.046\n356  356 3276.778\n357  357 3299.131\n358  358 3233.809\n359  359 3267.359\n360  360 3308.945\n361  361 3258.015\n362  362 3238.911\n363  363 3183.232\n364  364 3148.439\n365  365 3309.507\n366  366 3450.301\n367  367 3473.524\n368  368 3562.122\n369  369 3646.875\n370  370 3603.963\n371  371 3536.474\n372  372 3529.304\n373  373 3537.797\n374  374 3510.998\n375  375 3444.120\n376  376 3389.132\n377  377 3331.777\n378  378 3309.542\n379  379 3454.692\n380  380 3405.504\n381  381 3478.774\n382  382 3495.679\n383  383 3561.415\n384  384 3615.548\n385  385 3569.792\n386  386 3499.876\n387  387 3529.308\n388  388 3513.681\n389  389 3463.153\n390  390 3494.844\n391  391 3588.083\n392  392 3559.671\n393  393 3536.394\n394  394 3542.117\n395  395 3625.731\n396  396 3615.833\n397  397 3675.441\n398  398 3719.310\n399  399 3761.912\n400  400 3815.638\n401  401 3731.793\n402  402 3794.285\n403  403 3666.548\n404  404 3711.776\n405  405 3681.167\n406  406 3545.554\n407  407 3556.241\n408  408 3594.141\n409  409 3505.815\n410  410 3485.442\n411  411 3532.646\n412  412 3549.783\n413  413 3635.061\n414  414 3610.882\n415  415 3622.903\n416  416 3598.843\n417  417 3531.522\n418  418 3541.804\n419  419 3649.158\n420  420 3783.474\n421  421 3733.189\n422  422 3716.033\n423  423 3680.506\n424  424 3765.265\n425  425 3663.062\n426  426 3731.696\n427  427 3750.776\n428  428 3710.209\n429  429 3796.987\n430  430 3746.410\n431  431 3774.292\n432  432 3674.906\n433  433 3714.480\n434  434 3726.241\n435  435 3693.360\n436  436 3808.601\n437  437 3889.969\n438  438 3959.425\n439  439 3989.853\n440  440 3949.081\n441  441 4026.981\n442  442 4092.341\n443  443 3988.255\n444  444 3897.537\n445  445 3962.780\n446  446 3950.064\n447  447 3898.208\n448  448 3754.236\n449  449 3836.288\n450  450 3764.678\n451  451 3841.363\n452  452 3769.657\n453  453 3694.786\n454  454 3686.297\n455  455 3749.374\n456  456 3740.541\n457  457 3804.533\n458  458 3840.985\n459  459 3882.067\n460  460 3946.211\n461  461 3941.317\n462  462 3993.469\n463  463 4010.452\n464  464 4108.620\n465  465 4029.250\n466  466 4015.104\n467  467 3932.103\n468  468 3944.298\n469  469 4039.882\n470  470 4118.914\n471  471 4142.223\n472  472 4160.581\n473  473 4005.445\n474  474 3981.078\n475  475 4065.821\n476  476 4077.463\n477  477 4105.638\n478  478 4152.592\n479  479 4089.412\n480  480 4152.131\n481  481 4192.142\n482  482 4262.264\n483  483 4249.392\n484  484 4130.558\n485  485 4028.389\n486  486 4069.358\n487  487 4006.919\n488  488 4031.771\n489  489 4101.061\n490  490 4080.252\n491  491 4015.474\n492  492 4088.432\n493  493 3963.662\n494  494 3997.699\n495  495 3970.981\n496  496 4060.007\n497  497 4128.588\n498  498 4122.829\n499  499 4012.549\n500  500 3892.622"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#question-3",
    "href": "projects/project1/hw1_questions.html#question-3",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Question 3:",
    "text": "Question 3:\nWhat is the stocking quantity at Week 301?\n\n# set exogenous values as constants\nprice &lt;- 4\ncost &lt;- 0.8\nsalvage &lt;- 0\nSL &lt;- (price-cost)/(price-salvage)\n\n# Add the forecast to the dataframe\ndf &lt;- df %&gt;%\n  dplyr::mutate(forecast_Holt = Ft[1:nrow(df)],\n                forecast_error_Holt = demand - forecast_Holt)\n\n# Calculate safety stock and stocking quantity for Week 301\nsafety_stock &lt;- as.numeric(quantile(df$forecast_error_Holt[1:300], SL, na.rm=TRUE))\nstocking_optimal_adaptive &lt;- Ft[301] + safety_stock\n\n# Print the stocking quantity at Week 301\ncat(sprintf(\"The stocking quantity at Week 301 is: %.2f\", stocking_optimal_adaptive))\n\nThe stocking quantity at Week 301 is: 3737.22"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#question-4",
    "href": "projects/project1/hw1_questions.html#question-4",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Question 4:",
    "text": "Question 4:\nWhat is the weekly average profit from Week301 to Week500? Do you see an improvement over simple Exponential Smoothing?\n\n# set placeholder table\nprofit &lt;- matrix(NA, nrow = nrow(df), ncol = 4)\ncolnames(profit) &lt;- c(\"Oracle\", \"Simple ES\", \"Holt\", \"Holt (tuned)\")\nrownames(profit) &lt;- paste0(\"day\", 1:nrow(df))\nhead(profit)\n\n     Oracle Simple ES Holt Holt (tuned)\nday1     NA        NA   NA           NA\nday2     NA        NA   NA           NA\nday3     NA        NA   NA           NA\nday4     NA        NA   NA           NA\nday5     NA        NA   NA           NA\nday6     NA        NA   NA           NA\n\n\n\n# oracle\n# we are assuming we have actual demand\nfor(i in 301:nrow(df)) {\n  profit[i, \"Oracle\"] &lt;- price*df$demand[i]-cost*df$demand[i]\n}\n\n\n# Simple ES\nL0 &lt;- 2000\nalpha &lt;- 0.2\nLt &lt;- c(); Ft &lt;- c()\nfor (i in 1:nrow(df)) {\n  if(i==1) {\n    Lt[i] &lt;- alpha*df$demand[i]+(1-alpha)*L0 \n    Ft[i] &lt;- L0\n  } else {\n    Lt[i] &lt;- alpha*df$demand[i]+(1-alpha)*Lt[i-1]\n    Ft[i] &lt;- Lt[i-1]\n  }\n}\ndf &lt;- df %&gt;%\n  dplyr::mutate(forecast_Simple_ES=Ft,\n                forecast_error_Simple_ES= demand - forecast_Simple_ES)\n\n# finding safety stock based on TSL\nfor (i in 301:nrow(df)) {\n  safety_stock &lt;- as.numeric(quantile(df$forecast_error_Simple_ES[1:(i-1)],SL,na.rm=TRUE))\n  stocking_optimal_adaptive&lt;- df$forecast_Simple_ES[i] + safety_stock\n  profit[i,\"Simple ES\"] &lt;-\n    price*min(stocking_optimal_adaptive,df$demand[i]) -\n    cost*stocking_optimal_adaptive\n}\n\n\n# Holt\nfor (i in 301:nrow(df)) {\n  safety_stock &lt;- as.numeric(quantile(df$forecast_error_Holt[1:(i-1)], SL, na.rm=TRUE))\n  stocking_optimal_adaptive &lt;- df$forecast_Holt[i] + safety_stock\n  profit[i, \"Holt\"] &lt;-\n    price * min(stocking_optimal_adaptive, df$demand[i]) -\n    cost * stocking_optimal_adaptive\n}\n\naverage_profit &lt;- mean(profit[301:500, \"Holt\"], na.rm=TRUE)\n\n# Print the weekly average profit\ncat(sprintf(\"The weekly average profit from Week 301 to Week 500 is: %.2f\", average_profit))\n\nThe weekly average profit from Week 301 to Week 500 is: 11284.45\n\n\nWhen we calculate the average profit for a Simple Exponential Smoothing and our Holt model, we observe that the Holt model does not offer an improvement over the Simple ES.\n\n# compare average profit by forecasting method\napply(profit[301:500, ], 2, mean)\n\n      Oracle    Simple ES         Holt Holt (tuned) \n    11620.86     11298.89     11284.45           NA"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#question-5",
    "href": "projects/project1/hw1_questions.html#question-5",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Question 5",
    "text": "Question 5\nUse the data from Week1 to Week300 to find an “optimal” alpha and beta values.\n\n# Function to calculate MSE for given alpha and beta\nmse_holt &lt;- function(params, predict_from, predict_to, df, L0, T0) {\n  alpha &lt;- params[1]\n  beta &lt;- params[2]\n  \n  df_predict &lt;- df %&gt;%\n    dplyr::slice(predict_from:predict_to)\n  df_true &lt;- df %&gt;%\n    dplyr::slice(predict_from:predict_to)\n  \n  Lt &lt;- c()\n  Tt &lt;- c()\n  Ft &lt;- c()\n  n &lt;- predict_to - predict_from + 1\n  \n  for (i in 1:n) {\n    if(i == 1) {\n      Lt[i] &lt;- alpha * df_predict$demand[i] + (1 - alpha) * (L0 + T0)\n      Tt[i] &lt;- beta * (Lt[i] - L0) + (1 - beta) * T0\n      Ft[i] &lt;- L0 + T0\n    } else {\n      Lt[i] &lt;- alpha * df_predict$demand[i] + (1 - alpha) * (Lt[i-1] + Tt[i-1])\n      Tt[i] &lt;- beta * (Lt[i] - Lt[i-1]) + (1 - beta) * Tt[i-1]\n      Ft[i] &lt;- Lt[i-1] + Tt[i-1]\n    }\n  }\n  \n  mse &lt;- sum((df_true$demand - Ft)^2) / n\n  return(mse)\n}\n\n\n# Check the initial MSE value\ninitial_mse &lt;- mse_holt(c(0.2, 0.2), 1, 300, df, L0, T0)\ncat(sprintf(\"The initial MSE is: %.2f\\n\", initial_mse))\n\nThe initial MSE is: 111095.64\n\n\n\n# Optimizing alpha and beta using the optim function\nres &lt;- optim(\n  par = c(0.2, 0.2), \n  fn = mse_holt, \n  method = \"L-BFGS-B\",\n  predict_from = 1,\n  predict_to = 300,\n  df = df,\n  L0 = L0,\n  T0 = T0,\n  lower = c(0.01,0.0),\n  upper=c(1,1),\n  control = list(maxit = 1000)\n)\n\nres\n\n$par\n[1] 0.013468860 0.000111058\n\n$value\n[1] 88371.42\n\n$counts\nfunction gradient \n      49       49 \n\n$convergence\n[1] 0\n\n$message\n[1] \"CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\"\n\n\n\n  # Extract optimal alpha and beta\n  optimal_alpha &lt;- res$par[1]\n  optimal_beta &lt;- res$par[2]\n  \n  # Print the optimal alpha and beta values\n  cat(sprintf(\"The optimal alpha is: %.2f\\n\", optimal_alpha))\n\nThe optimal alpha is: 0.01\n\n  cat(sprintf(\"The optimal beta is: %.2f\\n\", optimal_beta))\n\nThe optimal beta is: 0.00"
  },
  {
    "objectID": "projects/project1/hw1_questions.html#question-6",
    "href": "projects/project1/hw1_questions.html#question-6",
    "title": "Forecasting Demand: An Analysis of Different Methods",
    "section": "Question 6:",
    "text": "Question 6:\nUse alpha and beta values from Question 5, and repeat Question 4.\n\n# Initialize level, trend, and forecast for Holt-Winters with optimal parameters\nLt &lt;- c(); Tt &lt;- c(); Ft &lt;- c()\n\n# Holt-Winters forecasting with optimal parameters\nfor (i in 1:500) {\n  if(i == 1) {\n    Lt[i] &lt;- optimal_alpha * df$demand[i] + (1 - optimal_alpha) * (L0 + T0)\n    Tt[i] &lt;- optimal_beta * (Lt[i] - L0) + (1 - optimal_beta) * T0\n    Ft[i] &lt;- L0 + T0\n  } else if (i &lt;= nrow(df)) {\n    Lt[i] &lt;- optimal_alpha * df$demand[i] + (1 - optimal_alpha) * (Lt[i-1] + Tt[i-1])\n    Tt[i] &lt;- optimal_beta * (Lt[i] - Lt[i-1]) + (1 - optimal_beta) * Tt[i-1]\n    Ft[i] &lt;- Lt[i-1] + Tt[i-1]\n  } else {\n    Lt[i] &lt;- Lt[i-1] + Tt[i-1]\n    Tt[i] &lt;- Tt[i-1]\n    Ft[i] &lt;- Lt[i-1] + Tt[i-1]\n  }\n}\n\n# Add the forecast to the dataframe\ndf &lt;- df %&gt;%\n  dplyr::mutate(forecast_Holt_tuned = Ft[1:nrow(df)],\n                forecast_error_Holt_tuned = demand - forecast_Holt_tuned)\n\n# Calculate Holt-Winters profit with optimal parameters\nfor (i in 301:nrow(df)) {\n  safety_stock &lt;- as.numeric(quantile(df$forecast_error_Holt_tuned[1:(i-1)], SL, na.rm=TRUE))\n  stocking_optimal_adaptive &lt;- df$forecast_Holt_tuned[i] + safety_stock\n  profit[i, \"Holt (tuned)\"] &lt;- price * min(stocking_optimal_adaptive, df$demand[i]) - cost * stocking_optimal_adaptive\n}\n\n# Compare average profit by forecasting method\naverage_profits &lt;- apply(profit[301:500, ], 2, mean, na.rm=TRUE)\nprint(average_profits)\n\n      Oracle    Simple ES         Holt Holt (tuned) \n    11620.86     11298.89     11284.45     11315.78"
  },
  {
    "objectID": "mgta456_project2/hw2.html",
    "href": "mgta456_project2/hw2.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment tests a variety of hypotheses, including whether matching donations increases the likelihood of a donation, whether the size of the match matters, and whether the size of the donation is affected by the match. To accomplish this, the authors used a variety of treatments, varying the price ratios for the match, the maximum matching grant amount, and the individual-specific ask amount. The treatments were letters used to solicit donations for a liberal politically motivated group that were sent to all 50 states. To control for spatial heterogeneity, the authors made sure that the treatment and control groups were balanced across states using demographic data, state and county election data, and data on the organization’s activity level within each state. Ultimately, the authors find that matching donations increases the likelihood of a donation, however, they also find that the size of the match does not matter, and that the size of the donation is not affected by the match.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "mgta456_project2/hw2.html#introduction",
    "href": "mgta456_project2/hw2.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment tests a variety of hypotheses, including whether matching donations increases the likelihood of a donation, whether the size of the match matters, and whether the size of the donation is affected by the match. To accomplish this, the authors used a variety of treatments, varying the price ratios for the match, the maximum matching grant amount, and the individual-specific ask amount. The treatments were letters used to solicit donations for a liberal politically motivated group that were sent to all 50 states. To control for spatial heterogeneity, the authors made sure that the treatment and control groups were balanced across states using demographic data, state and county election data, and data on the organization’s activity level within each state. Ultimately, the authors find that matching donations increases the likelihood of a donation, however, they also find that the size of the match does not matter, and that the size of the donation is not affected by the match.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "mgta456_project2/hw2.html#data",
    "href": "mgta456_project2/hw2.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n# read the data \nlibrary(haven)\ndata &lt;- read_dta(\"/home/jovyan/My_Quarto_Website/projects/project1/karlan_list_2007.dta\")\n\n# summarize treatment/control split\ntreatment_perc &lt;- mean(data$treatment)\ncontrol_perc &lt;-mean(data$control)\n\n# Print the result\nprint(treatment_perc)\n\n[1] 0.6668131\n\nprint(control_perc)\n\n[1] 0.3331869\n\n\n\n# Create tables to show the distributions of the treatment features\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nratio_counts &lt;- data %&gt;%\n  count(ratio) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100,2))\n\nsize_counts &lt;- data %&gt;%\n  count(size) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100,2))\n\nask_counts &lt;- data %&gt;%\n  count(ask) %&gt;%\n  mutate(percentage = round(n / sum(n) * 100,2))\n\n# Print the result\nprint(ratio_counts)\n\n# A tibble: 4 × 3\n  ratio           n percentage\n  &lt;dbl+lbl&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 0 [Control] 16687       33.3\n2 1           11133       22.2\n3 2           11134       22.2\n4 3           11129       22.2\n\nprint(ask_counts)\n\n# A tibble: 4 × 3\n  ask             n percentage\n  &lt;dbl+lbl&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 0 [Control] 16687       33.3\n2 1 [1x]      11134       22.2\n3 2 [1.25x]   11133       22.2\n4 3 [1.50x]   11129       22.2\n\nprint(size_counts)\n\n# A tibble: 5 × 3\n  size             n percentage\n  &lt;dbl+lbl&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 0 [Control]  16687       33.3\n2 1 [$25,000]   8350       16.7\n3 2 [$50,000]   8345       16.7\n4 3 [$100,000]  8350       16.7\n5 4 [Unstated]  8351       16.7\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWe test for a difference in means using both the t-test method and separately the linear regression method. In this case, we use both methods to confirm what we are seeing, however, both methods will ultimately produce the same result. Comparing the means of both the couple and pwhite variables respectively, we see that there is no statistically significant difference between the observations in the treatment and control groups. We tested this hypothesis for each variable at a 95% confidence level. These results give us the confidence to conclude that we do not have the statistical evidence to reject the null hypothesis that there is no difference in the means between the two groups.\n\nComparing Means for couple\n\n#split data into treatment and control\ncontrol_df &lt;- data[data$treatment == 0, ]\ntreatment_df &lt;- data[data$treatment == 1, ]\n\n# create data for ttest\nmean_control &lt;- mean(control_df$couple, na.rm = TRUE)\nsd_control &lt;- sd(control_df$couple, na.rm = TRUE)\nN_control &lt;- length(control_df$couple)\n\nmean_treatment &lt;- mean(treatment_df$couple, na.rm = TRUE)\nsd_treatment &lt;- sd(treatment_df$couple, na.rm = TRUE)\nN_treatment &lt;- length(treatment_df$couple)\n  \n# calculate tstat\nt_stat &lt;- (mean_control - mean_treatment) / sqrt((sd_control^2 / N_control) + (sd_treatment^2 / N_treatment))\nprint(t_stat)\n\n[1] 0.5888429\n\n\n\ncouple_lm &lt;- lm(couple~treatment, data=data)\nsummary(couple_lm)\n\n\nCall:\nlm(formula = couple ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09297 -0.09297 -0.09136 -0.09136  0.90864 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.092975   0.002261  41.124   &lt;2e-16 ***\ntreatment   -0.001617   0.002770  -0.584    0.559    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2889 on 48933 degrees of freedom\n  (1148 observations deleted due to missingness)\nMultiple R-squared:  6.965e-06, Adjusted R-squared:  -1.347e-05 \nF-statistic: 0.3408 on 1 and 48933 DF,  p-value: 0.5594\n\n\n\n\nComparing Means for pwhite\n\n#split data into treatment and control\ncontrol_df &lt;- data[data$treatment == 0, ]\ntreatment_df &lt;- data[data$treatment == 1, ]\n\n# create data for ttest\nmean_control &lt;- mean(control_df$pwhite, na.rm = TRUE)\nsd_control &lt;- sd(control_df$pwhite, na.rm = TRUE)\nN_control &lt;- length(control_df$pwhite)\n\nmean_treatment &lt;- mean(treatment_df$pwhite, na.rm = TRUE)\nsd_treatment &lt;- sd(treatment_df$pwhite, na.rm = TRUE)\nN_treatment &lt;- length(treatment_df$pwhite)\n  \n# calculate tstat\nt_stat &lt;- (mean_control - mean_treatment) / sqrt((sd_control^2 / N_control) + (sd_treatment^2 / N_treatment))\nprint(t_stat)\n\n[1] 0.570052\n\n\n\n\nLinear Regression for pwhite\n\npwhite_lm &lt;- lm(pwhite~treatment, data=data)\nsummary(pwhite_lm)\n\n\nCall:\nlm(formula = pwhite ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81079 -0.06378  0.05300  0.11939  0.18070 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.8202078  0.0013309  616.28   &lt;2e-16 ***\ntreatment   -0.0009128  0.0016292   -0.56    0.575    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1686 on 48215 degrees of freedom\n  (1866 observations deleted due to missingness)\nMultiple R-squared:  6.51e-06,  Adjusted R-squared:  -1.423e-05 \nF-statistic: 0.3139 on 1 and 48215 DF,  p-value: 0.5753"
  },
  {
    "objectID": "mgta456_project2/hw2.html#experimental-results",
    "href": "mgta456_project2/hw2.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nUsing a barplot to visually represen the outcome, we first observe the proportion of people who donated using one bar for treatment and one bar for control. Based on the bar plot below, there is a clear difference in the outcome: the treatement group has a higher proportion of people who donated than the control group. The barplot gives us reason to believe that the treatment may have a causal impact on the outcome of giving. Next, we test this hypothesis using statistical methods.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Calculating proportions\ndonation_summary &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(proportion_donated = mean(gave))\n\n# Ensure 'treatment' is a factor\ndonation_summary$treatment &lt;- as.factor(donation_summary$treatment)\n\n# Plotting the proportions\nggplot(donation_summary, aes(x = treatment, y = proportion_donated, fill = treatment)) +\n  geom_col() +  \n  scale_fill_manual(values = c(\"blue\", \"red\")) + \n  labs(title = \"Proportion of People Who Donated by Treatment Status\",\n       x = \"Treatment Status\",\n       y = \"Proportion that Donated\") +\n  theme_minimal()  \n\n\n\n\n\n\n\n\nWe can test our hypothesis by using a t-test to compare the proportions observed between the treatment and control groups. As we have done before, we will confirm the results from our t-test using a bivariate linear regression to demonstrate the same finding.\nThe hypothesis we seek to test is whether or not the proportion of people who gave is the same in both the treatment and the control groups. The result from our t-test is that the treatment group is substantially different from the control group. A large t-stat(t-stat &gt; 2) tells us that we have evidence to suggest that the groups differ and in our case, the t-stat is greater than 3. The results from our bivariate linear regression confirm this finding, adding that being in the treatment group increases the probability of making a charitable donation by 0.004 percent. The third model we use to confirm our results is a probit model. The probit model confirms wht we see in both the t-test as well as the linear regression, allowing us to conclude that we have sufficient statistical evidence to conclude that the treatment has a positive effect on the likelihood of giving.\n\nT-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\n#split data into treatment and control\ncontrol_df &lt;- data[data$treatment == 0, ]\ntreatment_df &lt;- data[data$treatment == 1, ]\n\n# create data for ttest\nmean_control &lt;- mean(control_df$gave, na.rm = TRUE)\nsd_control &lt;- sd(control_df$gave, na.rm = TRUE)\nN_control &lt;- length(control_df$gave)\n\nmean_treatment &lt;- mean(treatment_df$gave, na.rm = TRUE)\nsd_treatment &lt;- sd(treatment_df$gave, na.rm = TRUE)\nN_treatment &lt;- length(treatment_df$gave)\n  \n# calculate tstat\nt_stat &lt;- (mean_control - mean_treatment) / sqrt((sd_control^2 / N_control) + (sd_treatment^2 / N_treatment))\nprint(t_stat)\n\n[1] -3.209462\n\n\n\n\nBi-Variate Linear Regression for whether any charitable donation was made.\n\ngave_lm &lt;- lm(gave~treatment, data=data)\nsummary(gave_lm)\n\n\nCall:\nlm(formula = gave ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\n\n# Fit the probit regression model\nprobit &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\nsummary(probit)\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nWe use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. Our results show that the match threshold does not have a statistically significant effect on whether or not people donate. Although this result goes against conventional wisdom, the finding aligns with what was observed in the study.\n\n# Perform a t-test on outcome1\ntest12 &lt;- t.test(gave ~ ratio, data = subset(data, ratio %in% c(1, 2)))\ntest13 &lt;- t.test(gave ~ ratio, data = subset(data, ratio %in% c(1, 3)))\ntest23 &lt;- t.test(gave ~ ratio, data = subset(data, ratio %in% c(2, 3)))\n\n# Print results\nprint(test12)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 1 mean in group 2 \n     0.02074912      0.02263338 \n\nprint(test13)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 1 mean in group 3 \n     0.02074912      0.02273340 \n\nprint(test23)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 2 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 2 mean in group 3 \n     0.02263338      0.02273340 \n\n\nTo test the claim that the match threshold has no significant impact on whether or not someone decides to give, we execute a logistic regression, regressing gave on ratio1, ratio2, and ratio3. This test will tell us how each ratio effects the odds of someone giving anything. The coefficient for ratio 1 is positive but not statistically significant, meaning that the effect of the 1:1 ratio is not statistically significant from the effect observed when no match is offered. The effect of ratio’s 2 and 3 are both positive and statistically significant. Notably, their coefficients are nearly exactly the same, which shows that the difference between a 2:1 ratio and a 3:1 ratio is essentially the same.\n\n# Create 'ratio1' \ndata &lt;- data %&gt;%\n  mutate(ratio1 = as.integer(ratio == 1))\n\n# Fit a logistic regression model\nmodel &lt;- lm(gave ~ ratio1 + ratio2 + ratio3, data = data)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = gave ~ ratio1 + ratio2 + ratio3, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02263 -0.02075 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\nratio1      0.002891   0.001740   1.661  0.09662 .  \nratio2      0.004775   0.001740   2.744  0.00606 ** \nratio3      0.004875   0.001740   2.802  0.00509 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50079 degrees of freedom\nMultiple R-squared:  0.0002195, Adjusted R-squared:  0.0001596 \nF-statistic: 3.665 on 3 and 50079 DF,  p-value: 0.01176\n\n\n\n# create data subsets\nratio1_data &lt;- data[data$ratio == 1, ]\nratio2_data &lt;- data[data$ratio == 2, ]\nratio3_data &lt;- data[data$ratio == 3, ]\n\n# Calculate the mean of 'gave' for the subsets\nratio1_rr &lt;- mean(ratio1_data$gave, na.rm = TRUE) \nratio2_rr &lt;- mean(ratio2_data$gave, na.rm = TRUE) \nratio3_rr &lt;- mean(ratio3_data$gave, na.rm = TRUE) \n\nprint(ratio1_rr)\n\n[1] 0.02074912\n\nprint(ratio2_rr)\n\n[1] 0.02263338\n\nprint(ratio3_rr)\n\n[1] 0.0227334\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\namount_lm &lt;- lm(amount~treatment, data=data)\nsummary(amount_lm)\n\n\nCall:\nlm(formula = amount ~ treatment, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\nRepeating the analysis above using only the data from people who made a contribution, we are able to measure how much respondents donate conditional on donating some positive amount. In this case, we see that being in the treatment group has a negative impact, however, this impact is not statistically significant. This tells us that we are unsure whether the treatment is actually effecting the amount that people decide to give. Becuase our treatment coefficient is insignificant, we cannot conclude that the treatment has any effect on the amount that people decide to give. For this reason, we conclude that we do not have sufficient evidence to state that the treatment has a causal effect on the outcome.\n\ngivers_only &lt;- data[data$amount &gt; 0, ]\namount_lm &lt;- lm(amount~treatment, data=givers_only)\nsummary(amount_lm)\n\n\nCall:\nlm(formula = amount ~ treatment, data = givers_only)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\n\n# control group givers\ngivers_control &lt;- control_df[control_df$amount &gt; 0, ]\n\n# Calculate the mean of the donation amounts\nmean_amount &lt;- mean(givers_control$amount)\n\n# Creating the histogram with a vertical line indicating the mean\nhistogram1 &lt;- ggplot(givers_control, aes(x = amount)) +\n  geom_histogram(binwidth = 10, fill = \"blue\", color = \"black\") +  # Histogram\n  geom_vline(aes(xintercept = mean_amount), color = \"red\", linetype = \"dashed\", size = 1) +  # Mean line\n  labs(title = \"Donation Amounts (Control Group)\",\n       x = \"Donation Amount\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  ggtitle(paste(\"Control Group Donation Amounts with Mean at $\", round(mean_amount, 2)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Print the histogram\nprint(histogram1)\n\n\n\n\n\n\n\n\n\n# treatment group givers\ngivers_treatment &lt;- treatment_df[treatment_df$amount &gt; 0, ]\n\n# Calculate the mean of the donation amounts\nmean_amount &lt;- mean(givers_treatment$amount)\n\n# Creating the histogram with a vertical line indicating the mean\nhistogram1 &lt;- ggplot(givers_treatment, aes(x = amount)) +\n  geom_histogram(binwidth = 10, fill = \"blue\", color = \"black\") +  # Histogram\n  geom_vline(aes(xintercept = mean_amount), color = \"red\", linetype = \"dashed\", size = 1) +  # Mean line\n  labs(title = \"Donation Amounts (Treatment Group)\",\n       x = \"Donation Amount\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  ggtitle(paste(\"Treatment Group Donation Amounts with Mean at $\", round(mean_amount, 2)))\n\n# Print the histogram\nprint(histogram1)"
  },
  {
    "objectID": "mgta456_project2/hw2.html#simulation-experiment",
    "href": "mgta456_project2/hw2.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThis first plot is a demonstration of the Law of Large Numbers. This law states that as your sample size increases, you can expect your sample average to get closer to the true mean. To recreate this, we first simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. Next, we calculate a vector of 10,000 differences and then plot the cumulative average of that vector of differences. The plot shows that as we increase our sample size, the sample mean gets closer to the true mean. In our case, the sample mean never actually approaches the true difference in means. One simple way to fix this would be to increase our sample size beyond 10,000.\n\n# number of simulations\nn &lt;- 10000\n\n# probability of success\np_c &lt;- 0.018\np_t &lt;- 0.022\n\n# True mean difference\ntrue_diff &lt;- p_t - p_c\n\n# simulated n draws for treatment and control\nset.seed(10)\ncontrol_sim &lt;- rbinom(n,size = 1, prob = p_c)\ntreatment_sim &lt;- rbinom(n,size = 1, prob = p_t)\n\n# Calculate cumulative means for both simulations\ncummean_control &lt;- cumsum(control_sim) / (1:n)\ncummean_treatment &lt;- cumsum(treatment_sim) / (1:n)\n\n# Calculate the cumulative differences\nsample_cumdiff &lt;- cummean_treatment - cummean_control\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Simulation = 1:n,\n  CumulativeDifference = sample_cumdiff\n)\n\n\n# Generate the plot\nggplot(plot_data, aes(x = Simulation, y = CumulativeDifference)) +\n  geom_line(color = \"red\") + \n  geom_hline(yintercept = true_diff, linetype = \"dashed\", color = \"blue\") + \n  labs(title = \"Cumulative Difference Between Treatment and Control\",\n       x = \"Number of Simulations\",\n       y = \"Cumulative Difference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nTo demonstrate the Central Limit Theorem we create histograms showing us the average difference between the treatment and control group from simulations of different sample sizes. The sample sizes we tested are 50, 200, 500, and 1000. As you can see in the plots, there are less observations in the tails of the data as the sample size increases. This observation validates the Central Limit Theorem because it shows that as we increase sample size the sample statistic gets closer to the true value. It is also worth noting that the value zero is in the middle of the distribution in each histogram.\n\n# Number of repetitions\nreps &lt;- 1000\n\n# Different numbers of draws\ndraw_sizes &lt;- c(50, 200, 500, 1000)\n\n# Probabilities for control and treatment groups\np_c &lt;- 0.018\np_t &lt;- 0.022\n\n# Set a seed for reproducibility\nset.seed(10)\n\n# Loop over each draw size\nfor (draws in draw_sizes) {\n    # Initialize a vector to store the average differences for the current draw size\n    mean_diff &lt;- numeric(reps)\n    \n    # Repeat draw and calculation for the current number of draws\n    for (i in 1:reps) {\n        # Simulate draws from Bernoulli distributions for control and treatment\n        c_sim &lt;- rbinom(draws, size = 1, prob = p_c)\n        t_sim &lt;- rbinom(draws, size = 1, prob = p_t)\n    \n        # Calculate the average difference and store it\n        mean_diff[i] &lt;- mean(t_sim) - mean(c_sim)\n    }\n    \n    # Visualize the distribution of average differences for the current draw size\n    plot_data &lt;- data.frame(AverageDifferences = mean_diff)\n    p &lt;- ggplot(plot_data, aes(x = AverageDifferences)) +\n        geom_histogram(bins = 30, fill = \"dodgerblue\", color = \"black\") +\n        labs(title = paste(\"Histogram of Average Differences for\", draws, \"Draws\"),\n             x = \"Average Difference\",\n             y = \"Frequency\") +\n        theme_minimal()\n    \n    print(p)  # Display the plot\n}"
  },
  {
    "objectID": "projects/project4/hw4_questions.html",
    "href": "projects/project4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\n# Load necessary library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(lm.beta)\nlibrary(relaimpo)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: boot\n\n\nLoading required package: survey\n\n\nLoading required package: grid\n\n\nLoading required package: Matrix\n\n\nLoading required package: survival\n\n\n\nAttaching package: 'survival'\n\n\nThe following object is masked from 'package:boot':\n\n    aml\n\n\n\nAttaching package: 'survey'\n\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n\n\nLoading required package: mitools\n\n\nThis is the global version of package relaimpo.\n\n\nIf you are a non-US user, a version with the interesting additional metric pmvd is available\n\n\nfrom Ulrike Groempings web site at prof.beuth-hochschule.de/groemping.\n\nlibrary(iml)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n# Read data into data fram\ndata &lt;- read.csv(\"~/My_Quarto_Website/projects/project4/data_for_drivers_analysis.csv\")"
  },
  {
    "objectID": "projects/project4/hw4_questions.html#pearson-correlation",
    "href": "projects/project4/hw4_questions.html#pearson-correlation",
    "title": "Key Drivers Analysis",
    "section": "Pearson Correlation",
    "text": "Pearson Correlation\nPearson correlation is a measure of the linear relationship between two variables. It quantifies the degree to which changes in one variable predict changes in another, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation). While Pearson correlation is straightforward and easy to interpret, it is sensitive to multicollinearity. When predictor variables are highly correlated, it becomes difficult to disentangle their individual effects on the outcome variable. This can lead to misleading conclusions about the importance of each predictor, as the correlation might reflect the combined influence of multiple correlated predictors rather than a single predictor’s effect.\n\n#pearson correlations\n\n# Assuming your data is loaded into a dataframe called 'data'\n# Calculate the Pearson correlation matrix\ncorrelation_matrix &lt;- cor(data, method = \"pearson\")\n\n# Extract the Pearson correlation values for 'satisfaction' with each variable\nsatisfaction_correlations &lt;- correlation_matrix['satisfaction', ]\nsatisfaction_correlations &lt;- satisfaction_correlations[!names(satisfaction_correlations) %in% c('satisfaction', 'brand', 'id')]\n\n# Convert the correlation values to percentages and round them to tenths\nsatisfaction_correlations &lt;- round(satisfaction_correlations * 100, 1)\n\n# Create a dataframe with the Pearson correlation values\nsatisfaction_correlations_df &lt;- data.frame(\n  Perception = names(satisfaction_correlations),\n  `Pearson correlation` = satisfaction_correlations,\n  row.names = NULL\n)\n\n# Print the dataframe\nprint(satisfaction_correlations_df)\n\n  Perception Pearson.correlation\n1      trust                25.6\n2      build                19.2\n3    differs                18.5\n4       easy                21.3\n5  appealing                20.8\n6  rewarding                19.5\n7    popular                17.1\n8    service                25.1\n9     impact                25.5"
  },
  {
    "objectID": "projects/project4/hw4_questions.html#standardized-regression-coefficient",
    "href": "projects/project4/hw4_questions.html#standardized-regression-coefficient",
    "title": "Key Drivers Analysis",
    "section": "Standardized Regression Coefficient",
    "text": "Standardized Regression Coefficient\nThe standardized regression coefficient, also known as the beta coefficient, indicates how many standard deviations the dependent variable will change per standard deviation increase in the predictor variable. This standardization allows for direct comparison of the relative importance of predictors in a regression model. However, in the presence of multicollinearity, standardized regression coefficients can be unstable and inflated. Multicollinearity causes large standard errors, making it difficult to ascertain the true effect of each predictor. Consequently, even though standardized coefficients provide a sense of relative importance, their reliability diminishes when predictors are highly correlated.\n\n# standardized regression coefficients\n\n# Run a linear regression with the original variables\nmodel &lt;- lm(satisfaction ~ ., data = data)\n\n# Extract the unstandardized coefficients\nunstandardized_coefficients &lt;- coef(model)\n\n# Remove intercept\nunstandardized_coefficients &lt;- unstandardized_coefficients[-1]\n\n# Compute the variance of each predictor and the response variable\nvariances &lt;- sapply(data, var)\n\n# Compute the standardized coefficients using the specified method\nstandardized_coefficients &lt;- unstandardized_coefficients * \n                             variances[names(unstandardized_coefficients)] / \n                             variances[\"satisfaction\"]\n\n# Create a dataframe with the standardized regression coefficients\nstandardized_coefficients_df &lt;- data.frame(\n  Perception = names(standardized_coefficients),\n  `Standardized regression coefficient` = round(standardized_coefficients * 100, 1)\n)\n\n# Assuming satisfaction_correlations_df is your existing dataframe with Pearson correlations\n# Merge the dataframes by the Perception column\nmerged_df &lt;- merge(satisfaction_correlations_df, standardized_coefficients_df, by = \"Perception\", all.x = TRUE)\n\n# Print the dataframe\nprint(merged_df)\n\n  Perception Pearson.correlation Standardized.regression.coefficient\n1  appealing                20.8                                 1.5\n2      build                19.2                                 0.8\n3    differs                18.5                                 1.1\n4       easy                21.3                                 1.0\n5     impact                25.5                                 5.2\n6    popular                17.1                                 0.7\n7  rewarding                19.5                                 0.3\n8    service                25.1                                 3.7\n9      trust                25.6                                 4.9"
  },
  {
    "objectID": "projects/project4/hw4_questions.html#usefulness",
    "href": "projects/project4/hw4_questions.html#usefulness",
    "title": "Key Drivers Analysis",
    "section": "Usefulness",
    "text": "Usefulness\nThe “usefulness” metric is designed to quantify the additional explanatory power that each predictor variable provides when included in the regression model. This is achieved by calculating the increase in the model’s R-squared value when each predictor is added last, after accounting for all other predictors. This approach is implemented in the relaimpo package under the name “last” and can also be computed directly using the drop1 function in R. The usefulness metric addresses multicollinearity by evaluating each predictor’s contribution in the context of the other predictors already in the model. When predictors are correlated, their individual contributions to the overall model fit might be smaller, and the usefulness metric typically sums to less than the total R-squared value of the model. This method helps to distribute the shared variance among correlated predictors, providing a clearer picture of each predictor’s unique importance despite the presence of multicollinearity.\n\n# \"usefulness\"\n\n# Calculate the relative importance using the \"last\" metric\nrelimp_results &lt;- calc.relimp(model, type = \"last\")\n\n# Extract the relative importance values\nusefulness_values &lt;- relimp_results$lmg\n\n# Create a dataframe with the relative importance values\nusefulness_df &lt;- data.frame(\n  Perception = names(usefulness_values),\n  Usefulness = round(usefulness_values * 100, 1)\n)\n\n# Merge the dataframes by the Perception column\nmerged_df &lt;- merge(satisfaction_correlations_df, standardized_coefficients_df, by = \"Perception\", all.x = TRUE)\n\n# Calculate the relative importance using the \"last\" metric\nrelimp_results &lt;- calc.relimp(model, type = \"lmg\", rela = FALSE)\n\n# Extract the relative importance values\nusefulness_values &lt;- relimp_results$lmg\n\n# Create a dataframe with the relative importance values\nusefulness_df &lt;- data.frame(\n  Perception = names(usefulness_values),\n  Usefulness = round(usefulness_values * 100, 1),\n  row.names = NULL\n)\n\n# Merge the usefulness values into the existing dataframe\nfinal_df &lt;- merge(merged_df, usefulness_df, by = \"Perception\", all.x = TRUE)\n\n# Print the final dataframe\nprint(final_df)\n\n  Perception Pearson.correlation Standardized.regression.coefficient Usefulness\n1  appealing                20.8                                 1.5        0.9\n2      build                19.2                                 0.8        0.7\n3    differs                18.5                                 1.1        0.7\n4       easy                21.3                                 1.0        0.9\n5     impact                25.5                                 5.2        2.4\n6    popular                17.1                                 0.7        0.6\n7  rewarding                19.5                                 0.3        0.7\n8    service                25.1                                 3.7        1.8\n9      trust                25.6                                 4.9        2.2"
  },
  {
    "objectID": "projects/project4/hw4_questions.html#shapley-value",
    "href": "projects/project4/hw4_questions.html#shapley-value",
    "title": "Key Drivers Analysis",
    "section": "Shapley Value",
    "text": "Shapley Value\nShapley values, derived from cooperative game theory, offer a fair distribution of the total contribution to the predictive power of a model among all predictors. Each predictor’s importance is assessed by considering all possible subsets of predictors, ensuring that each predictor’s contribution is fairly allocated. Shapley values are robust to multicollinearity because they consider all possible combinations of predictors, effectively averaging out the influence of correlated predictors. This makes Shapley values a powerful method for understanding variable importance in complex models with multicollinearity.\n\n# shapley values for linear regression\n# Calculate Shapley values using the iml package\n# Create a Predictor object\npredictor &lt;- Predictor$new(model, data = data, y = data$satisfaction)\n\n# Calculate Shapley values for the first instance\nshapley &lt;- Shapley$new(predictor, x.interest = data[1, ])\n\n# Aggregate Shapley values for each feature\nshapley_values &lt;- aggregate(shapley$results$phi, by = list(shapley$results$feature), FUN = mean)\ncolnames(shapley_values) &lt;- c(\"Perception\", \"Shapley value\")\n\n# Convert Shapley values to percentages and round them to tenths\nshapley_values$`Shapley value` &lt;- round(shapley_values$`Shapley value` * 100, 1)\n\n# Merge the Shapley values into the existing dataframe\nfinal_df &lt;- merge(merged_df, shapley_values, by = \"Perception\", all.x = TRUE)\n\n# Print the final dataframe\nprint(final_df)\n\n  Perception Pearson.correlation Standardized.regression.coefficient\n1  appealing                20.8                                 1.5\n2      build                19.2                                 0.8\n3    differs                18.5                                 1.1\n4       easy                21.3                                 1.0\n5     impact                25.5                                 5.2\n6    popular                17.1                                 0.7\n7  rewarding                19.5                                 0.3\n8    service                25.1                                 3.7\n9      trust                25.6                                 4.9\n  Shapley value\n1           4.3\n2          -2.0\n3           5.4\n4           2.6\n5         -10.3\n6          -1.9\n7          -0.8\n8          10.5\n9          11.4"
  },
  {
    "objectID": "projects/project4/hw4_questions.html#johnsons-relative-weights",
    "href": "projects/project4/hw4_questions.html#johnsons-relative-weights",
    "title": "Key Drivers Analysis",
    "section": "Johnson’s Relative Weights",
    "text": "Johnson’s Relative Weights\nJohnson’s relative weights measure the proportion of variance in the dependent variable explained by each predictor, considering the unique contribution of each predictor while accounting for multicollinearity. This method decomposes the total R-squared into parts attributable to each predictor, helping to identify which predictors are most important in the presence of correlated predictors. By using principal component analysis, Johnson’s relative weights effectively distribute the shared variance among correlated predictors, offering a more accurate representation of each predictor’s importance. This makes it a valuable tool for addressing multicollinearity and understanding variable importance in regression models.\n\n# johnsons relative weights\n# Calculate Johnson's relative weights using the reilaimpo package\nrelimp_results &lt;- calc.relimp(model, type = \"lmg\", rela = FALSE)\n\n# Extract Johnson's relative weights\njohnsons_relative_weights &lt;- relimp_results$lmg\n\n# Create a dataframe with Johnson's relative weights\njohnsons_relative_weights_df &lt;- data.frame(\n  Perception = names(johnsons_relative_weights),\n  `Johnson's relative weight` = round(johnsons_relative_weights * 100, 1),\n  row.names = NULL\n)\n\n# Assuming final_df is your existing dataframe with Pearson correlations, standardized coefficients, relative importance, and Shapley values\n# Merge Johnson's relative weights into the existing dataframe\nfinal_df &lt;- merge(final_df, johnsons_relative_weights_df, by = \"Perception\", all.x = TRUE)\n\n# Print the final dataframe\nprint(final_df)\n\n  Perception Pearson.correlation Standardized.regression.coefficient\n1  appealing                20.8                                 1.5\n2      build                19.2                                 0.8\n3    differs                18.5                                 1.1\n4       easy                21.3                                 1.0\n5     impact                25.5                                 5.2\n6    popular                17.1                                 0.7\n7  rewarding                19.5                                 0.3\n8    service                25.1                                 3.7\n9      trust                25.6                                 4.9\n  Shapley value Johnson.s.relative.weight\n1           4.3                       0.9\n2          -2.0                       0.7\n3           5.4                       0.7\n4           2.6                       0.9\n5         -10.3                       2.4\n6          -1.9                       0.6\n7          -0.8                       0.7\n8          10.5                       1.8\n9          11.4                       2.2\n\n\n\n# mean decrease in random forest gini coefficient\n\n# Fit a Random Forest model\n#set.seed(123) # For reproducibility\n#rf_model &lt;- randomForest(satisfaction ~ ., data = data, importance = TRUE)\n\n# Inspect the importance output to identify the correct column names\n#importance_output &lt;- importance(rf_model)\n#print(importance_output)\n\n# Extract Mean Decrease in Gini coefficients\n#importance_df &lt;- as.data.frame(importance_output)\n#importance_df$Perception &lt;- rownames(importance_df)\n#rownames(importance_df) &lt;- NULL\n\n# Print importance_df to verify the column names\n#print(importance_df)\n\n# Extract only the Mean Decrease in Gini values\n# The correct column name should be identified from the inspection\n#mean_decrease_gini_df &lt;- importance_df[, c(\"Perception\", \"MeanDecreaseGini\")]\n\n# Round the Mean Decrease in Gini values\n#mean_decrease_gini_df$MeanDecreaseGini &lt;- round(mean_decrease_gini_df$MeanDecreaseGini, 1)\n\n# Assuming final_df is your existing dataframe with Pearson correlations, standardized coefficients, relative importance, and Shapley values\n# Merge Mean Decrease in Gini coefficients into the existing dataframe\n#final_df &lt;- merge(final_df, mean_decrease_gini_df, by = \"Perception\", all.x = TRUE)\n\n# Print the final dataframe\n#print(final_df)\n\n\n# Change variable names\nfinal_df$Perception[final_df$Perception == 'trust'] &lt;- \"Is offered by a brand I trust\"\nfinal_df$Perception[final_df$Perception == 'build'] &lt;- \"Helps build credit quickly\"\nfinal_df$Perception[final_df$Perception == 'differs'] &lt;- \"Is different from other cards\"\nfinal_df$Perception[final_df$Perception == 'easy'] &lt;- \"Is easy to use\"\nfinal_df$Perception[final_df$Perception == 'appealing'] &lt;- \"Has appealing benefits or rewards\"\nfinal_df$Perception[final_df$Perception == 'rewarding'] &lt;- \"Rewards me for responsible usage\"\nfinal_df$Perception[final_df$Perception == 'popular'] &lt;- \"Is used by a lot of people\"\nfinal_df$Perception[final_df$Perception == 'service'] &lt;- \"Provides outstanding customer service\"\nfinal_df$Perception[final_df$Perception == 'impact'] &lt;- \"Makes a difference in my life\"\n\nprint(final_df)\n\n                             Perception Pearson.correlation\n1     Has appealing benefits or rewards                20.8\n2            Helps build credit quickly                19.2\n3         Is different from other cards                18.5\n4                        Is easy to use                21.3\n5         Makes a difference in my life                25.5\n6            Is used by a lot of people                17.1\n7      Rewards me for responsible usage                19.5\n8 Provides outstanding customer service                25.1\n9         Is offered by a brand I trust                25.6\n  Standardized.regression.coefficient Shapley value Johnson.s.relative.weight\n1                                 1.5           4.3                       0.9\n2                                 0.8          -2.0                       0.7\n3                                 1.1           5.4                       0.7\n4                                 1.0           2.6                       0.9\n5                                 5.2         -10.3                       2.4\n6                                 0.7          -1.9                       0.6\n7                                 0.3          -0.8                       0.7\n8                                 3.7          10.5                       1.8\n9                                 4.9          11.4                       2.2\n\n\nIf you want a challenge, either (1) implement one or more of the measures yourself. “Usefulness” is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost."
  }
]